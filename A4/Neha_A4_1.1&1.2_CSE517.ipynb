{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "\n",
    "# The first time you run this will download a ~823MB file\n",
    "glove = torchtext.vocab.GloVe(name=\"6B\", # trained on Wikipedia 2014 corpus\n",
    "                              dim=100)    # embedding size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/45113130/how-to-add-new-embeddings-for-unknown-words-in-tensorflow-training-pre-set-fo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchtext.vocab import Vectors\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = glove['cat']\n",
    "# y = glove['dog']\n",
    "# torch.norm(y - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1., 1., 1.]).unsqueeze(0)\n",
    "y = torch.tensor([2., 2., 2.]).unsqueeze(0)\n",
    "torch.cosine_similarity(x, y) # should be one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2118])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = glove['england']\n",
    "y = glove['beer']\n",
    "torch.cosine_similarity(glove['england'].unsqueeze(0),\n",
    "                        glove['beer'].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = glove['England']\n",
    "y = glove['wines ']\n",
    "torch.cosine_similarity(glove['England'].unsqueeze(0),\n",
    "                        glove['wines'].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A4_ 1 Word Similarity\n",
    "Now that we have a notion of distance in our embedding space, we can talk about words that are \"close\" to each other in the embedding space. For now, let's use Euclidean distances to look at how close various words are to the word \"ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat 2.6811304092407227\n",
      "puppy 3.9500551223754883\n",
      "kitten 5.06204080581665\n",
      "mouse 5.034541130065918\n",
      "kite 6.637244701385498\n",
      "lion 5.573644638061523\n",
      "doggy 6.244095802307129\n"
     ]
    }
   ],
   "source": [
    "word = 'dog'\n",
    "other = ['cat', 'puppy', 'kitten', 'mouse', 'kite', 'lion', 'doggy']\n",
    "for w in other:\n",
    "    dist = torch.norm(glove[word] - glove[w]) # euclidean distance\n",
    "    print(w, float(dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, we can look through our entire vocabulary for words that are closest to a point in the embedding space -- for example, we can look for words that are closest to another word like \"cat\". (You did this in project 2!)\n",
    "\n",
    "Keep in mind that GloVe vectors are trained on word co-occurrences, and so words with similar embeddings will tend to co-occur with other words. For example, \"cat\" and \"dog\" tend to occur with similar other words---even more so than \"cat\" and \"kitten\" because these two words tend to occur in different contexts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat 2.6811304\n",
      "dogs 3.2425272\n",
      "puppy 3.9500551\n",
      "pet 3.9634414\n",
      "horse 4.3288527\n"
     ]
    }
   ],
   "source": [
    "def print_closest_words(vec, n=5):\n",
    "    dists = torch.norm(glove.vectors - vec, dim=1)     # compute distances to all words\n",
    "    lst = sorted(enumerate(dists.numpy()), key=lambda x: x[1]) # sort by distance\n",
    "    for idx, difference in lst[1:n+1]: \t\t\t\t\t       # take the top n\n",
    "        print(glove.itos[idx], difference)\n",
    "\n",
    "print_closest_words(glove[\"dog\"], n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shark 3.657539\n",
      "dolphin 3.7841108\n",
      "whales 4.000368\n",
      "humpback 4.200512\n",
      "hunts 4.511149\n"
     ]
    }
   ],
   "source": [
    "print_closest_words(glove['whale'], n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 2.2805324\n",
      "when 2.6176212\n",
      "again 2.9376447\n",
      "then 2.9955394\n",
      "time 3.0521593\n"
     ]
    }
   ],
   "source": [
    "print_closest_words(glove['before'], n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "although 1.2712862\n",
      "though 1.8014803\n",
      "but 2.6178648\n",
      ". 2.6181011\n",
      "nevertheless 2.6530395\n"
     ]
    }
   ],
   "source": [
    "print_closest_words(glove['however'], n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invent 3.2982993\n",
      "embellish 3.4175081\n",
      "synthesise 3.6254725\n",
      "falsify 3.636186\n",
      "mass-produce 3.6533558\n"
     ]
    }
   ],
   "source": [
    "print_closest_words(glove['fabricate'], n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A4_2  Analogies\n",
    "One surprising aspect of GloVe vectors is that the directions in the embedding space can be meaningful. The structure of the GloVe vectors certain analogy-like relationship like this tend to hold:\n",
    "\n",
    "king−man+woman≈queen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kitten 3.8146477\n",
      "cat 3.9500551\n",
      "puppies 4.0255\n",
      "kittens 4.157487\n",
      "pterodactyl 4.1881576\n"
     ]
    }
   ],
   "source": [
    "print_closest_words(glove['puppy'] - glove['dog'] + glove['cat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speak 7.5174046\n",
      "sang 7.5502667\n",
      "cry 7.610008\n",
      "laugh 7.689598\n",
      "sings 7.704893\n"
     ]
    }
   ],
   "source": [
    "print_closest_words(glove['speak'] - glove['speaker'] + glove['sing'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scottish 4.189797\n",
      "english 4.2134395\n",
      "welsh 4.718583\n",
      "irish 4.8836055\n",
      "british 4.9753633\n"
     ]
    }
   ],
   "source": [
    "print_closest_words(glove['french'] - glove['france'] + glove['england'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wines 3.5871894\n",
      "tasting 4.2200603\n",
      "beer 4.5414815\n",
      "grape 4.6474686\n",
      "champagne 4.6741643\n"
     ]
    }
   ],
   "source": [
    "print_closest_words(glove['wine'] - glove['France'] + glove['England'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A4_3\n",
    "Select a text classification dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Module \n",
    "import os \n",
    "\n",
    "# Folder Path \n",
    "path = r'/Users/nehakardam/Documents/UWclasses /CSE NLP/A1/A1_data/review_polarity/txt_sentoken/pos'\n",
    "\n",
    "# Change the directory \n",
    "os.chdir(path) \n",
    "\n",
    "# Read text File \n",
    "def read_text_file(file_path): \n",
    "    with open(file_path, 'r') as f: \n",
    "        return f.read().splitlines()  \n",
    "\n",
    "# iterate through all file \n",
    "collection_of_data_pos = []\n",
    "for file in os.listdir(): \n",
    "    # Check whether file is in text format or not \n",
    "    if file.endswith(\".txt\"): \n",
    "        file_path = f\"{path}/{file}\"\n",
    "        # call read text file function \n",
    "        collection_of_data_pos = collection_of_data_pos + read_text_file(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos = pd.DataFrame(collection_of_data_pos, columns = ['Reviews'])\n",
    "df_pos['Sentiments'] = 'Positive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder Path \n",
    "path = r'/Users/nehakardam/Documents/UWclasses /CSE NLP/A1/A1_data/review_polarity/txt_sentoken/neg'\n",
    "\n",
    "# Change the directory \n",
    "os.chdir(path)   \n",
    "\n",
    "# iterate through all file \n",
    "collection_of_data_neg = []\n",
    "for file in os.listdir(): \n",
    "    # Check whether file is in text format or not \n",
    "    if file.endswith(\".txt\"): \n",
    "        file_path = f\"{path}/{file}\"\n",
    "        # call read text file function \n",
    "        collection_of_data_neg = collection_of_data_neg + read_text_file(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neg = pd.DataFrame(collection_of_data_neg, columns = ['Reviews'])\n",
    "df_neg['Sentiments'] = 'Negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Sentiments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>his explanation ?</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this obviously wasn't a big concern of the fil...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the twilight murders , another one from the se...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the matchmaker falls for every cliche of thing...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fortunately for chris kattan's precariously po...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64715</th>\n",
       "      <td>of course , crafting a tightly claustrophobic ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64716</th>\n",
       "      <td>it has no reason to it ; it's just bad directi...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64717</th>\n",
       "      <td>from that , they went on and created the most ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64718</th>\n",
       "      <td>a fine cast aside , \" pushing tin \" is nothing...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64719</th>\n",
       "      <td>meanwhile , cusack meets a business partner ( ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64720 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Reviews Sentiments\n",
       "0                                     his explanation ?    Positive\n",
       "1      this obviously wasn't a big concern of the fil...   Negative\n",
       "2      the twilight murders , another one from the se...   Negative\n",
       "3      the matchmaker falls for every cliche of thing...   Negative\n",
       "4      fortunately for chris kattan's precariously po...   Negative\n",
       "...                                                  ...        ...\n",
       "64715  of course , crafting a tightly claustrophobic ...   Negative\n",
       "64716  it has no reason to it ; it's just bad directi...   Negative\n",
       "64717  from that , they went on and created the most ...   Positive\n",
       "64718  a fine cast aside , \" pushing tin \" is nothing...   Negative\n",
       "64719  meanwhile , cusack meets a business partner ( ...   Positive\n",
       "\n",
       "[64720 rows x 2 columns]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#both the dataframe of positive and negative reviews are concatinated to one dataframe named 'collection_of_data'\n",
    "df_pos_neg = [df_pos, df_neg]\n",
    "Collection_of_data = pd.concat(df_pos_neg)\n",
    "#Shuffle the columns of the data\n",
    "All_data= Collection_of_data.sample(frac=1).reset_index(drop=True)\n",
    "All_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_data.to_csv('/Users/nehakardam/Documents/UWclasses /CSE NLP/A1/A1_data/review_polarity/txt_sentoken/All_data_jan2022.csv', index = False)\n",
    "lex_neg = pd.read_csv('/Users/nehakardam/Documents/UWclasses /CSE NLP/A1/A1_data/opinion_lexicon_English/neg-words.txt', header = None)\n",
    "lex_pos = pd.read_csv('/Users/nehakardam/Documents/UWclasses /CSE NLP/A1/A1_data/opinion_lexicon_English/pos-words.txt', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_neg_pos = {}\n",
    "for index, value in lex_neg[0].items():\n",
    "    dict_neg_pos[value] = \"negative\"\n",
    "for index, value in lex_pos[0].items():\n",
    "    dict_neg_pos[value] = \"positive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [and, by, putting, this, idea, in, the, settin...\n",
       "1        [however, the, closing, 20, minutes, of, the, ...\n",
       "2        [the, only, saving, grace, for, friday, the, 1...\n",
       "3        [this, is, a, very, freudian, way, to, look, a...\n",
       "4        [the, film, poses, as, many, questions, as, an...\n",
       "                               ...                        \n",
       "64715    [nary, a, mention, of, these, apocalyptic, eve...\n",
       "64716    [having, not, read, the, novel, it, s, difficu...\n",
       "64717    [they, re, never, even, directly, connected, t...\n",
       "64718    [roberta, s, mother, is, initially, reluctant,...\n",
       "64719                                           [at, best]\n",
       "Name: Reviews_Tokenized, Length: 64720, dtype: object"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def tokenizer(theText):\n",
    "    theTokens = re.findall(r'\\b\\w[\\w-]*\\b', theText.lower())\n",
    "    return theTokens\n",
    "\n",
    "reviews_tokenized = []\n",
    "reviews = All_data['Reviews']\n",
    "\n",
    "for index, value in reviews.items():\n",
    "    reviews_tokenized.append(tokenizer(value))\n",
    "All_data['Reviews_Tokenized'] = reviews_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Sentiments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>his explanation ?</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this obviously wasn't a big concern of the fil...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the twilight murders , another one from the se...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the matchmaker falls for every cliche of thing...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fortunately for chris kattan's precariously po...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Reviews Sentiments\n",
       "0                                 his explanation ?    Positive\n",
       "1  this obviously wasn't a big concern of the fil...   Negative\n",
       "2  the twilight murders , another one from the se...   Negative\n",
       "3  the matchmaker falls for every cliche of thing...   Negative\n",
       "4  fortunately for chris kattan's precariously po...   Negative"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = All_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Sentiments', ylabel='count'>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.countplot(x='Sentiments', data=All_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(sen):\n",
    "    # Removing html tags\n",
    "    sentence = remove_tags(sen)\n",
    "\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAG_RE = re.compile(r'<[^>]+>')\n",
    "\n",
    "def remove_tags(text):\n",
    "    return TAG_RE.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "sentences = list(All_data['Reviews'])\n",
    "for sen in sentences:\n",
    "    X.append(preprocess_text(sen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.2910)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting the dat into train, validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = np.split(df.sample(frac=1), [int(.6*len(df)), int(.8*len(df))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Reviews']\n",
    "all_strings = list(map(str.strip,train['Reviews']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for words in glove:\n",
    "#     for word in reviews_tokenized[10]:\n",
    "#          if word not in glove:\n",
    "#             print (word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackabuse.com/python-for-nlp-movie-sentiment-analysis-using-deep-learning-in-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Activation, Dropout, Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import GlobalMaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64720, 3)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
