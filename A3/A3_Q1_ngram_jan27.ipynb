{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "\n",
    "class Ngram:\n",
    "    def __init__(self, N):\n",
    "        self.N = N\n",
    "        self.onegram_prob = None\n",
    "        self.bigram_prob = None\n",
    "        self.trigram_prob = None\n",
    "\n",
    "        # ONEGRAM ADD ONE SMOOTHING\n",
    "        self.one_gram_add_one_prob = None\n",
    "\n",
    "        # UNIGRAM GOOD TURING RESULTS - NOT USED, TESTING!!!\n",
    "        self.unigram_good_turing = None\n",
    "        self.unigram_zero_occurence_prob = None\n",
    "        self.unigram_good_turing_cstar = None\n",
    "\n",
    "        # BIGRAM GOOD TURING RESULTS\n",
    "        self.bigram_good_turing = None\n",
    "        self.bigram_zero_occurence_prob = None\n",
    "        self.bigram_good_turing_cstar = None\n",
    "\n",
    "        # TRIGRAM GOOD TURING RESULTS\n",
    "        self.trigram_good_turing = None\n",
    "        self.trigram_zero_occurence_prob = None\n",
    "        self.trigram_good_turing_cstar = None\n",
    "\n",
    "        # BIGRAM DISCOUNTED\n",
    "        self.bigram_discounted = None\n",
    "        self.bigram_saved_probability_mass = dict()\n",
    "\n",
    "        # TRIGRAM DISCOUNTED\n",
    "        self.trigram_discounted = None\n",
    "        self.trigram_saved_probability_mass = dict()\n",
    "\n",
    "    def train(self, sentences):\n",
    "        listOfBigrams = []\n",
    "        bigramCounts = {}\n",
    "        unigramCounts = {}\n",
    "\n",
    "        listOfTrigrams = []\n",
    "        trigramCounts = {}\n",
    "\n",
    "        for sentence in sentences:\n",
    "            words = sentence.split()\n",
    "            for i in range(len(words)):\n",
    "                if i < len(words) - 1:\n",
    "                    listOfBigrams.append((words[i], words[i + 1]))\n",
    "                    if (words[i], words[i + 1]) in bigramCounts:\n",
    "                        bigramCounts[(words[i], words[i + 1])] += 1\n",
    "                    else:\n",
    "                        bigramCounts[(words[i], words[i + 1])] = 1\n",
    "\n",
    "                    if i < len(words) - 2:\n",
    "                        listOfTrigrams.append((words[i], words[i + 1], words[i + 2]))\n",
    "                        if (words[i], words[i + 1], words[i + 2]) in trigramCounts:\n",
    "                            trigramCounts[(words[i], words[i + 1], words[i + 2])] += 1\n",
    "                        else:\n",
    "                            trigramCounts[(words[i], words[i + 1], words[i + 2])] = 1\n",
    "\n",
    "                if words[i] in unigramCounts:\n",
    "                    unigramCounts[words[i]] += 1\n",
    "                else:\n",
    "                    unigramCounts[words[i]] = 1\n",
    "\n",
    "        return listOfBigrams, unigramCounts, bigramCounts, listOfTrigrams, trigramCounts\n",
    "\n",
    "    def prob(self, sentence):\n",
    "        '''Returns the MLE probability of the given sentence. '''\n",
    "        if self.N == 3:\n",
    "            return self.get_trigram_prob(sentence)\n",
    "        elif self.N == 2:\n",
    "            return self.get_bigram_prob(sentence)\n",
    "        elif self.N == 1:\n",
    "            return self.get_onegram_prob(sentence)\n",
    "\n",
    "    def sprob(self, sentence):\n",
    "        '''Returns the smoothed probability of a given sentence. '''\n",
    "        if self.N == 3:\n",
    "            return self.get_trigram_good_turing_prob(sentence)\n",
    "        elif self.N == 2:\n",
    "            return self.get_bigram_good_turing_prob(sentence)\n",
    "        elif self.N == 1:\n",
    "            return self.get_onegram_add_one_prob(sentence)\n",
    "\n",
    "    def next(self, before_prior=None, prior=None):\n",
    "        '''Samples a word from the conditional distribution of given context.'''\n",
    "        if self.N == 3:\n",
    "            return self.trigram_generate_next_word(before_prior, prior)\n",
    "        elif self.N == 2:\n",
    "            return self.bigram_generate_next_word(prior)\n",
    "        elif self.N == 1:\n",
    "            return self.unigram_generate_next_word()\n",
    "\n",
    "    def ppl(self, sentences):\n",
    "        return self.ppl_unigram(sentences)\n",
    "\n",
    "    def get_onegram_prob(self, sentence):\n",
    "        '''Return onegram probabilty'''\n",
    "        score = 1\n",
    "        for word in sentence.split():\n",
    "            score *= self.onegram_prob.get(word, 0)\n",
    "        return score\n",
    "\n",
    "    def calculate_onegram_prob(self, unigram_counts):\n",
    "        '''Assigns onegram probabilties'''\n",
    "        onegram_prob = {}\n",
    "        total_word_count = 0\n",
    "        for elem in unigram_counts:\n",
    "            total_word_count += unigram_counts[elem]\n",
    "\n",
    "        for onegram in unigram_counts:\n",
    "            onegram_prob[onegram] = (unigram_counts.get(onegram)) / total_word_count\n",
    "\n",
    "        with open('../output/1gram.txt', 'w') as file:\n",
    "            file.write('Onegram' + '\\t\\t\\t' + 'Count' + '\\t' + 'Probability' + '\\n')\n",
    "            for unigram in unigram_counts:\n",
    "                file.write(str(unigram) + ' : ' + str(unigram_counts[unigram])\n",
    "                           + ' : ' + str(onegram_prob[unigram]) + '\\n')\n",
    "        self.onegram_prob = onegram_prob\n",
    "        return onegram_prob\n",
    "\n",
    "    def get_onegram_add_one_prob(self, sentence):\n",
    "        '''Returns onegram probabilty of the sentence'''\n",
    "        score = 1\n",
    "        for word in sentence.split():\n",
    "            score *= self.one_gram_add_one_prob.get(word, 0)\n",
    "        return score\n",
    "\n",
    "    def onegram_add_one_smothing(self, unigram_counts):\n",
    "        '''Assigns onegram probabilties with laplace smoohting, Here K=1, it can be adjusted'''\n",
    "        K = 1\n",
    "        one_gram_add_one_prob = {}\n",
    "        total_word_count = 0\n",
    "        for elem in unigram_counts:\n",
    "            total_word_count += unigram_counts[elem]\n",
    "\n",
    "        for onegram in unigram_counts:\n",
    "            one_gram_add_one_prob[onegram] = (unigram_counts.get(onegram) + K) / (total_word_count + K * len(unigram_counts))\n",
    "\n",
    "        with open('../output/1gram-add-one.txt', 'w') as file:\n",
    "            file.write('Onegram' + '\\t\\t\\t' + 'Count' + '\\t' + 'Probability' + '\\n')\n",
    "            for unigram in unigram_counts:\n",
    "                file.write(str(unigram) + ' : ' + str(unigram_counts[unigram])\n",
    "                           + ' : ' + str(one_gram_add_one_prob[unigram]) + '\\n')\n",
    "\n",
    "        self.one_gram_add_one_prob = one_gram_add_one_prob\n",
    "        return one_gram_add_one_prob\n",
    "\n",
    "    def get_bigram_prob(self, sentence):\n",
    "        '''Return bigram probabilty of the sentence'''\n",
    "        score = 1\n",
    "        words = sentence.split()\n",
    "        for i in range(len(words) - 1):\n",
    "            unit = (words[i], words[i + 1])\n",
    "            if unit in self.bigram_prob:\n",
    "                score *= self.bigram_prob[unit]\n",
    "            else:\n",
    "                score *= 0\n",
    "        return score\n",
    "\n",
    "    def get_bigram_good_turing_prob(self, sentence):\n",
    "        '''Return bigram probabilty of the sentence with good turing smoohting'''\n",
    "        score = 1\n",
    "        words = sentence.split()\n",
    "        for i in range(len(words) - 1):\n",
    "            unit = (words[i], words[i + 1])\n",
    "            if unit in self.bigram_good_turing:\n",
    "                score *= self.bigram_good_turing[unit]\n",
    "            else:\n",
    "                score *= self.bigram_zero_occurence_prob\n",
    "        return score\n",
    "\n",
    "    def calculate_bigram_prob(self, listOfBigrams, unigramCounts, bigramCounts):\n",
    "        '''Assigns bigram probabilties'''\n",
    "        bigram_prob = {}\n",
    "        for bigram in listOfBigrams:\n",
    "            bigram_prob[bigram] = (bigramCounts.get(bigram)) / (unigramCounts.get(bigram[0]))\n",
    "\n",
    "        with open('../output/2gram.txt', 'w') as file:\n",
    "            file.write('Bigram' + '\\t\\t\\t' + 'Count' + '\\t' + 'Probability' + '\\n')\n",
    "            for bigrams in listOfBigrams:\n",
    "                file.write(str(bigrams) + ' : ' + str(bigramCounts[bigrams])\n",
    "                           + ' : ' + str(bigram_prob[bigrams]) + '\\n')\n",
    "        self.bigram_prob = bigram_prob\n",
    "        return bigram_prob\n",
    "\n",
    "    def get_trigram_prob(self, sentence):\n",
    "        '''Returns trigram probabilty of the sentence'''\n",
    "        score = 1\n",
    "        words = sentence.split()\n",
    "        for i in range(len(words) - 2):\n",
    "            unit = (words[i], words[i + 1], words[i + 2])\n",
    "            if unit in self.trigram_prob:\n",
    "                score *= self.trigram_prob[unit]\n",
    "            else:\n",
    "                score *= 0\n",
    "        return score\n",
    "\n",
    "    def get_trigram_good_turing_prob(self, sentence):\n",
    "        '''Returns trigram probabilty of the sentence with good turing smooting'''\n",
    "        score = 1\n",
    "        words = sentence.split()\n",
    "        for i in range(len(words) - 2):\n",
    "            unit = (words[i], words[i + 1], words[i + 2])\n",
    "            if unit in self.trigram_good_turing:\n",
    "                score *= self.trigram_good_turing[unit]\n",
    "            else:\n",
    "                score *= self.trigram_zero_occurence_prob\n",
    "        return score\n",
    "\n",
    "    def calculate_trigram_prob(self, list_of_trigrams, bigram_counts, trigram_counts):\n",
    "        '''Assigns trigram probabilties'''\n",
    "        trigram_prob = {}\n",
    "        for trigram in list_of_trigrams:\n",
    "            trigram_prob[trigram] = (trigram_counts.get(trigram)) / (bigram_counts.get((trigram[0], trigram[1])))\n",
    "\n",
    "        with open('../output/3gram.txt', 'w') as file:\n",
    "            file.write('Trigram' + '\\t\\t\\t' + 'Count' + '\\t' + 'Probability' + '\\n')\n",
    "            for bigrams in list_of_trigrams:\n",
    "                file.write(str(bigrams) + ' : ' + str(trigram_counts[bigrams])\n",
    "                           + ' : ' + str(trigram_prob[bigrams]) + '\\n')\n",
    "        self.trigram_prob = trigram_prob\n",
    "        return trigram_prob\n",
    "\n",
    "    def good_turing_smooting(self, list_of_word_unit, word_unit_counts, total_number_of_word_unit, filename_debug, filename_result):\n",
    "        '''Assigns good turing smoothed probabilties'''\n",
    "        list_of_probabilities = {}\n",
    "        bucket = {}\n",
    "        c_star = {}\n",
    "        p_star = {}\n",
    "        list_of_counts = {}\n",
    "\n",
    "        for word_unit in word_unit_counts.items():\n",
    "            value = word_unit[1]  # Of time occurs\n",
    "            if not value in bucket:\n",
    "                bucket[value] = 1\n",
    "            else:\n",
    "                bucket[value] += 1\n",
    "\n",
    "        bucket_list = sorted(bucket.items(), key=lambda t: t[0])\n",
    "        zero_occurence_prob = bucket_list[0][1] / total_number_of_word_unit  # This many words occurs only once\n",
    "        last_item = bucket_list[len(bucket_list) - 1][0]  # Most 266 occurs, occurs only one time\n",
    "\n",
    "        # Set non existing # of words\n",
    "        for x in range(1, last_item):\n",
    "            if x not in bucket:\n",
    "                bucket[x] = 0\n",
    "\n",
    "        bucket_list = sorted(bucket.items(), key=lambda t: t[0])\n",
    "        bucket_list_len = len(bucket_list)\n",
    "\n",
    "        i = 1\n",
    "        file = open(filename_debug, 'w')\n",
    "        file.write(\"#NumberOfOccurences\\t\\t\\tFrequency\\n\")\n",
    "        for k, v in bucket_list:\n",
    "            file.write(str(k) + \" : \" + str(v) + \"\\n\")\n",
    "            if i < bucket_list_len:\n",
    "                if v == 0:\n",
    "                    c_star[k] = 0\n",
    "                    p_star[k] = 0\n",
    "\n",
    "                else:\n",
    "                    c_star[k] = (i + 1) * bucket_list[i][1] / v\n",
    "                    p_star[k] = c_star[k] / total_number_of_word_unit\n",
    "\n",
    "            else:\n",
    "                c_star[k] = 0\n",
    "                p_star[k] = 0\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        file.close()\n",
    "        for word_unit in list_of_word_unit:\n",
    "            list_of_probabilities[word_unit] = p_star.get(word_unit_counts[word_unit])\n",
    "            list_of_counts[word_unit] = c_star.get(word_unit_counts[word_unit])\n",
    "\n",
    "        with open(filename_result, 'w') as file:\n",
    "            file.write('Word Unit' + '\\t\\t\\t' + 'Count' + '\\t' + 'Probability' + '\\n')\n",
    "\n",
    "            for bigrams in list_of_word_unit:\n",
    "                file.write(str(bigrams) + ' : ' + str(word_unit_counts[bigrams])\n",
    "                           + ' : ' + str(list_of_probabilities[bigrams]) + '\\n')\n",
    "\n",
    "        return list_of_probabilities, zero_occurence_prob, list_of_counts\n",
    "\n",
    "    def unigram_generate_next_word(self):\n",
    "        '''Genereates new word - unigram'''\n",
    "        choose_list = list(self.onegram_prob.keys())\n",
    "        choose_list.remove(\"<s>\")\n",
    "        while True:\n",
    "            return random.choice(choose_list)\n",
    "\n",
    "    def unigram_generate_sentence(self, maximum_iteration=20):\n",
    "        '''Genereates a sentence using unigrams'''\n",
    "        sent = [\"<s>\"]\n",
    "        for index in range(0, maximum_iteration):\n",
    "            word = self.unigram_generate_next_word()\n",
    "            sent.append(word)\n",
    "            if word == \"</s>\":\n",
    "                break\n",
    "        if index == maximum_iteration - 1:\n",
    "            sent.append(\"</s>\")\n",
    "        return sent\n",
    "\n",
    "    def unigram_generate_sentences(self, number_of_sentences, maximum_iteration=20):\n",
    "        '''Genereates some sentences using unigrams'''\n",
    "        sentences = []\n",
    "        for i in range(0, number_of_sentences):\n",
    "            sentence = self.unigram_generate_sentence(maximum_iteration)\n",
    "            sentences.append(sentence)\n",
    "        return sentences\n",
    "\n",
    "    def bigram_generate_next_word(self, prior):\n",
    "        '''Genereates new word - bigram'''\n",
    "        choose_list = []\n",
    "        for elem in list(self.bigram_prob.keys()):\n",
    "            if elem[0] == prior:\n",
    "                choose_list.append(elem[1])\n",
    "        while True:\n",
    "            return random.choice(choose_list)\n",
    "\n",
    "    def bigram_generate_sentence(self, maximum_iteration=20):\n",
    "        '''Genereates a sentence using bigrams'''\n",
    "        sent = []\n",
    "        word = \"<s>\"\n",
    "        sent.append(word)\n",
    "        for index in range(0, maximum_iteration):\n",
    "            word = self.bigram_generate_next_word(word)\n",
    "            sent.append(word)\n",
    "            if word == \"</s>\":\n",
    "                break\n",
    "        if index == maximum_iteration - 1:\n",
    "            sent.append(\"</s>\")\n",
    "        return sent\n",
    "\n",
    "    def bigram_generate_sentences(self, number_of_sentences, maximum_iteration=20):\n",
    "        '''Genereates some sentences using bigrams'''\n",
    "        sentences = []\n",
    "        for i in range(0, number_of_sentences):\n",
    "            sentence = self.bigram_generate_sentence(maximum_iteration)\n",
    "            sentences.append(sentence)\n",
    "        return sentences\n",
    "\n",
    "    def trigram_generate_next_word(self, prior_before, prior):\n",
    "        '''Genereates new word - trigram'''\n",
    "        choose_list = []\n",
    "        for elem in list(self.trigram_prob.keys()):\n",
    "            if elem[0] == prior_before:\n",
    "                if elem[1] == prior:\n",
    "                    choose_list.append(elem[2])\n",
    "        while True:\n",
    "            return random.choice(choose_list)\n",
    "\n",
    "    def trigram_generate_sentence(self, maximum_iteration=20):\n",
    "        '''Genereates a sentence using trigrams'''\n",
    "        sent = []\n",
    "        word = \"<s>\"\n",
    "        next_word = self.bigram_generate_next_word(word)\n",
    "        sent.append(word)\n",
    "        sent.append(next_word)\n",
    "        for index in range(0, maximum_iteration):\n",
    "            word = self.trigram_generate_next_word(sent[index], sent[index + 1])\n",
    "            sent.append(word)\n",
    "            if word == \"</s>\":\n",
    "                break\n",
    "        if index == maximum_iteration - 1:\n",
    "            sent.append(\"</s>\")\n",
    "        return sent\n",
    "\n",
    "    def trigram_generate_sentences(self, number_of_sentences, maximum_iteration=20):\n",
    "        '''Genereates some sentences using trigrams'''\n",
    "        sentences = []\n",
    "        for i in range(0, number_of_sentences):\n",
    "            sentence = self.trigram_generate_sentence(maximum_iteration)\n",
    "            sentences.append(sentence)\n",
    "        return sentences\n",
    "\n",
    "    def ppl_unigram(self, sentences):\n",
    "        '''Returns the unigram perplexity of the given list of sentences.'''\n",
    "        counter = 0\n",
    "        tmp = 0\n",
    "        for i in range(len(sentences)):\n",
    "            sentence = sentences[i]\n",
    "            for j in range(len(sentence)):\n",
    "                prob = self.onegram_prob.get(sentence[j], 0)\n",
    "                if prob == 0:\n",
    "                    tmp += 0\n",
    "                else:\n",
    "                    tmp += math.log(prob) / math.log(2)\n",
    "                counter += 1\n",
    "        # entropy = prob of each token / number of tokens\n",
    "        entropy = -1 / counter * tmp\n",
    "        # print(\"entropy\", entropy)\n",
    "        perplexity = math.pow(2, entropy)\n",
    "        return perplexity\n",
    "\n",
    "    def ppl_unigram_smoohted(self, sentences):\n",
    "        '''Returns the smoothed unigram perplexity of the given list of sentences. Here smoothed values are used.'''\n",
    "        counter = 0\n",
    "        tmp = 0\n",
    "        for i in range(len(sentences)):\n",
    "            sentence = sentences[i]\n",
    "            for j in range(len(sentence)):\n",
    "                prob = self.one_gram_add_one_prob.get(sentence[j], 0)\n",
    "                if prob == 0:\n",
    "                    tmp += 0\n",
    "                else:\n",
    "                    tmp += math.log(prob) / math.log(2)\n",
    "                counter += 1\n",
    "        # entropy = prob of each token / number of tokens\n",
    "        entropy = -1 / counter * tmp\n",
    "        perplexity = math.pow(2, entropy)\n",
    "        return perplexity\n",
    "\n",
    "    def ppl_bigram(self, sentences):\n",
    "        '''Returns the bigram perplexity of the given list of sentences. Here smoothed values are used.'''\n",
    "        counter = 0\n",
    "        tmp = 0\n",
    "        for i in range(len(sentences)):\n",
    "            sentence = sentences[i]\n",
    "            for j in range(len(sentence) - 1):\n",
    "                unit = (sentence[j], sentence[j + 1])\n",
    "                if unit in self.bigram_prob:\n",
    "                    prob = self.bigram_prob[unit]\n",
    "                else:\n",
    "                    prob = 0\n",
    "                if prob == 0:\n",
    "                    tmp += 0\n",
    "                else:\n",
    "                    tmp += math.log(prob, 2)\n",
    "                counter += 1\n",
    "        # entropy = prob of each token / number of tokens\n",
    "        entropy = -1 / counter * tmp\n",
    "        perplexity = math.pow(2, entropy)\n",
    "        return perplexity\n",
    "\n",
    "    def ppl_trigram(self, sentences):\n",
    "        '''Returns the trigram perplexity of the given list of sentences. Here smoothed values are used.'''\n",
    "        counter = 0\n",
    "        tmp = 0\n",
    "        for i in range(len(sentences)):\n",
    "            sentence = sentences[i]\n",
    "            for j in range(len(sentence) - 2):\n",
    "                unit = (sentence[j], sentence[j + 1], sentence[j + 2])\n",
    "                if unit in self.trigram_prob:\n",
    "                    prob = self.trigram_prob[unit]\n",
    "                else:\n",
    "                    prob = 0\n",
    "                if prob == 0:\n",
    "                    tmp += 0\n",
    "                else:\n",
    "                    tmp += math.log(prob, 2)\n",
    "                counter += 1\n",
    "        # entropy = prob of each token / number of tokens\n",
    "        entropy = -1 / counter * tmp\n",
    "        perplexity = math.pow(2, entropy)\n",
    "        return perplexity\n",
    "\n",
    "    def ppl_interpolation(self, sentences, lambda_set):\n",
    "        '''Returns the interpolated perplexity of the given list of sentences. Here smoothed values are used.'''\n",
    "        counter = 0\n",
    "        tmp = 0\n",
    "        for i in range(len(sentences)):\n",
    "            sentence = sentences[i]\n",
    "            for j in range(len(sentence) - 2):\n",
    "                trigram_unit = (sentence[j], sentence[j + 1], sentence[j + 2])\n",
    "                if trigram_unit in self.trigram_prob:\n",
    "                    tri_gram_prob = self.trigram_prob[trigram_unit]\n",
    "                else:\n",
    "                    tri_gram_prob = 0\n",
    "\n",
    "                bigram_unit = (sentence[j], sentence[j + 1])\n",
    "                if bigram_unit in self.bigram_prob:\n",
    "                    bigram_prob = self.bigram_prob[bigram_unit]\n",
    "                else:\n",
    "                    bigram_prob = 0\n",
    "\n",
    "                unigram_prob = self.one_gram_add_one_prob.get(sentence[j], 0)\n",
    "\n",
    "                prob = lambda_set[0] * unigram_prob + lambda_set[1] * bigram_prob + lambda_set[2] * tri_gram_prob\n",
    "                if prob == 0:\n",
    "                    tmp += 0\n",
    "                else:\n",
    "                    tmp += math.log(prob, 2)\n",
    "                counter += 1\n",
    "        # entropy = prob of each token / number of tokens\n",
    "        entropy = -1 / counter * tmp\n",
    "        perplexity = math.pow(2, entropy)\n",
    "        return perplexity\n",
    "\n",
    "    def calculate_bigram_discounting(self, list_of_bigrams, bigram_counts, unigram_counts, β):\n",
    "        '''Bigram discounting'''\n",
    "        bigram_prob = {}\n",
    "        for bigram in list(list_of_bigrams):\n",
    "            count = (bigram_counts.get(bigram) - β)\n",
    "            if count < 0:\n",
    "                count = 0\n",
    "            bigram_prob[bigram] = count / unigram_counts.get(bigram[0])\n",
    "\n",
    "        filename = '../output/2gram_discounted_b=' + str(β) + '.txt'\n",
    "        with open(filename, 'w') as file:\n",
    "            file.write('Bigram' + '\\t\\t\\t' + 'Count' + '\\t' + 'Probability' + '\\n')\n",
    "            for bigrams in list_of_bigrams:\n",
    "                file.write(str(bigrams) + ' : ' + str(bigram_counts[bigrams])\n",
    "                           + ' : ' + str(bigram_prob[bigrams]) + '\\n')\n",
    "        self.bigram_discounted = bigram_prob\n",
    "\n",
    "        with open('../output/2gram_discounted_b=' + str(β) + '_saved_prob_mass.txt', 'w') as file:\n",
    "            file.write('Unigram' + '\\t\\t\\t' + 'Sum' + '\\t\\t\\t' + '1-Sum' + '\\n')\n",
    "            for unigram in set(unigram_counts):\n",
    "                total = 0\n",
    "                for elem in bigram_prob:\n",
    "                    if elem[0] == unigram:\n",
    "                        total += bigram_prob[elem]\n",
    "                self.bigram_saved_probability_mass[unigram] = 1 - total\n",
    "                file.write(unigram + \" : \" + str(total) + \" : \" + str(1 - total) + \"\\n\")\n",
    "        return bigram_prob\n",
    "\n",
    "    def calculate_trigram_discounting(self, list_of_trigrams, trigram_counts, bigram_counts, β):\n",
    "        '''Trigram discounting'''\n",
    "        trigram_prob = {}\n",
    "        for trigram in list_of_trigrams:\n",
    "            count = (trigram_counts.get(trigram) - β)\n",
    "            if count < 0:\n",
    "                count = 0\n",
    "            trigram_prob[trigram] = count / bigram_counts.get((trigram[0], trigram[1]))\n",
    "\n",
    "        filename = '../output/3gram_discounted_b=' + str(β) + '.txt'\n",
    "        with open(filename, 'w') as file:\n",
    "            file.write('Trigram' + '\\t\\t\\t' + 'Count' + '\\t' + 'Probability' + '\\n')\n",
    "            for bigrams in list_of_trigrams:\n",
    "                file.write(str(bigrams) + ' : ' + str(trigram_counts[bigrams])\n",
    "                           + ' : ' + str(trigram_prob[bigrams]) + '\\n')\n",
    "        self.trigram_discounted = trigram_prob\n",
    "\n",
    "        with open('../output/3gram_discounted_b=' + str(β) + '_saved_prob_mass.txt', 'w') as file:\n",
    "            file.write('Bigram' + '\\t\\t\\t' + 'Sum' + '\\t\\t\\t' + '1-Sum' + '\\n')\n",
    "            for bigram in set(bigram_counts):\n",
    "                total = 0\n",
    "                for elem in trigram_counts:\n",
    "                    if elem[0] == bigram[0] and elem[1] == bigram[1]:\n",
    "                        total += trigram_prob[elem]\n",
    "                file.write(str(bigram) + \" : \" + str(total) + \" : \" + str(1 - total) + \"\\n\")\n",
    "                self.trigram_saved_probability_mass[bigram] = total\n",
    "\n",
    "        return trigram_prob\n",
    "\n",
    "    def ppl_bigram_discounted(self, sentences, test_bigram_counts, size_of_test_corpus):\n",
    "        '''Returns the discounted bigram perplexity of the given list of sentences. Here smoothed values are used.'''\n",
    "        counter = 0\n",
    "        tmp = 0\n",
    "        for i in range(len(sentences)):\n",
    "            sentence = sentences[i]\n",
    "            for j in range(len(sentence) - 1):\n",
    "                unit = (sentence[j], sentence[j + 1])\n",
    "                if unit in self.bigram_discounted:\n",
    "                    prob = self.bigram_discounted[unit]\n",
    "                else:\n",
    "                    prob = test_bigram_counts[unit] / size_of_test_corpus\n",
    "                if prob == 0:\n",
    "                    tmp += 0\n",
    "                else:\n",
    "                    tmp += math.log(prob, 2)\n",
    "                counter += 1\n",
    "        # entropy = prob of each token / number of tokens\n",
    "        entropy = -1 / counter * tmp\n",
    "        perplexity = math.pow(2, entropy)\n",
    "        return perplexity\n",
    "\n",
    "    def ppl_trigram_discounted(self, sentences, test_trigram_counts, size_of_test_corpus):\n",
    "        '''Returns the discounted trigram perplexity of the given list of sentences. Here smoothed values are used.'''\n",
    "        counter = 0\n",
    "        tmp = 0\n",
    "        for i in range(len(sentences)):\n",
    "            sentence = sentences[i]\n",
    "            for j in range(len(sentence) - 2):\n",
    "                unit = (sentence[j], sentence[j + 1], sentence[j + 2])\n",
    "                if unit in self.trigram_discounted:\n",
    "                    prob = self.trigram_discounted[unit]\n",
    "                else:\n",
    "                    prob = test_trigram_counts[unit] / size_of_test_corpus\n",
    "                if prob == 0:\n",
    "                    tmp += 0\n",
    "                else:\n",
    "                    tmp += math.log(prob, 2)\n",
    "                counter += 1\n",
    "        # entropy = prob of each token / number of tokens\n",
    "        entropy = -1 / counter * tmp\n",
    "        perplexity = math.pow(2, entropy)\n",
    "        return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = open('/Users/nehakardam/Documents/UWclasses /CSE NLP/A3/data_A3/1b_benchmark.train.tokens', encoding = 'utf8')\n",
    "#file = open('/Users/nehakardam/Documents/UWclasses /CSE NLP/A3/data_A3/1b_benchmark.dev.tokens', encoding = 'utf8')\n",
    "# file = open('/Users/nehakardam/Documents/UWclasses /CSE NLP/A3/data_A3/1b_benchmark.test.tokens', encoding = 'utf8')\n",
    "# train = [l.strip() for l in file.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 23505\n",
      "Model perplexity: 1412.184\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = Path(\"/Users/nehakardam/Documents/UWclasses /CSE NLP/ngram-language-model/data\")\n",
    "train, test = load_data(path)\n",
    "\n",
    "lm = LanguageModel(train, 3)\n",
    "print(\"Vocabulary size: {}\".format(len(lm.vocab)))\n",
    "\n",
    "perplexity = lm.perplexity(test)\n",
    "print(\"Model perplexity: {:.3f}\".format(perplexity))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9599"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(freq_uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 9599\n",
      "Model perplexity: 537.947\n"
     ]
    }
   ],
   "source": [
    "lm = LanguageModel(train, 1)\n",
    "print(\"Vocabulary size: {}\".format(len(lm.vocab)))\n",
    "perplexity = lm.perplexity(train)\n",
    "print(\"Model perplexity: {:.3f}\".format(perplexity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 9599\n",
      "Model perplexity: 670.571\n"
     ]
    }
   ],
   "source": [
    "lm = LanguageModel(train, 2)\n",
    "print(\"Vocabulary size: {}\".format(len(lm.vocab)))\n",
    "perplexity = lm.perplexity(train)\n",
    "print(\"Model perplexity: {:.3f}\".format(perplexity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 9599\n",
      "Model perplexity: 1592.478\n"
     ]
    }
   ],
   "source": [
    "lm = LanguageModel(train, 3)\n",
    "print(\"Vocabulary size: {}\".format(len(lm.vocab)))\n",
    "perplexity = lm.perplexity(train)\n",
    "print(\"Model perplexity: {:.3f}\".format(perplexity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "unigram = preprocess(train,1)\n",
    "bigrams = nltk.ngrams(unigram, 2)\n",
    "trigrams = nltk.ngrams(unigram, 3)\n",
    "\n",
    "freq_uni = nltk.FreqDist(unigram)\n",
    "freq_bi = nltk.FreqDist(bigrams)\n",
    "freq_tri = nltk.FreqDist(trigrams)\n",
    "\n",
    "uni_k = list(freq_uni.keys())\n",
    "uni_v= list(freq_uni.values())\n",
    "bi_k = list(freq_bi.keys())\n",
    "bi_v= list(freq_bi.values())\n",
    "tri_k = list(freq_tri.keys())\n",
    "tri_v= list(freq_tri.values())\n",
    "\n",
    "bi_k_str = ['_'.join(i) for i in bi_k]\n",
    "tri_k_str = ['_'.join(i) for i in tri_k]\n",
    "unigram_count_dict = dict(zip(uni_k,uni_v))\n",
    "bigram_count_dict = dict (zip(bi_k_str,bi_v))\n",
    "trigram_count_dict = dict (zip(tri_k_str,tri_v))\n",
    "\n",
    "unigram_data_items = unigram_count_dict.items()\n",
    "unigram_data_list = list(unigram_data_items)\n",
    "unigram_df = pd.DataFrame(unigram_data_list,columns=['Unigram','count'])\n",
    "unigram_df = unigram_df.sort_values(by=['count'],ascending=False,ignore_index=True).head(10)\n",
    "plt.rcParams[\"figure.figsize\"] = (10,6)\n",
    "y=unigram_df['Unigram']\n",
    "x=unigram_df['count']\n",
    "plt.barh(y, x)\n",
    "plt.ylabel(\"Unigrams\")\n",
    "plt.xlabel(\"Frequency\") \n",
    "# plt.title(\"Bigram Frequency Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Interpolation Smoothing\n",
    "Reference:https://github.com/ErolOZKAN-/Language-Modelling/blob/master/src/ngram.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def compute_prob(freq, N):\n",
    "    print (\"processing\")\n",
    "    return { ngram: count / N for ngram, count in freq.items() }\n",
    "        \n",
    "def perplexity(ngrams, freq):\n",
    "    N = len(unigram)\n",
    "    prob = compute_prob(freq, N) \n",
    "    probabilities = [prob[ngram] for ngram in ngrams]\n",
    "    print(probabilities)\n",
    "    return math.exp((-1/N) * sum(map(math.log, probabilities)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{('<s>', 'BAGHDAD'): 2.0,\n",
       " ('BAGHDAD', '--'): 0.5,\n",
       " ('--', 'An'): 2.0,\n",
       " ('An', 'Iraqi'): 1.0,\n",
       " ('Iraqi', 'military'): 0.5,\n",
       " ('military', 'commander'): 1.5,\n",
       " ('commander', 'on'): 0.5,\n",
       " ('on', 'Monday'): 31.0,\n",
       " ('Monday', 'compared'): 0.5,\n",
       " ('compared', 'new'): 0.5,\n",
       " ('new', 'restrictions'): 0.5,\n",
       " ('restrictions', 'on'): 1.0,\n",
       " ('on', 'the'): 263.0,\n",
       " ('the', 'U.S.'): 58.0,\n",
       " ('U.S.', 'military'): 6.5,\n",
       " ('military', 'to'): 0.5,\n",
       " ('to', '\"'): 13.0,\n",
       " ('\"', 'house'): 0.5,\n",
       " ('house', 'arrest'): 1.5,\n",
       " ('arrest', ','): 1.5,\n",
       " (',', '\"'): 445.0,\n",
       " ('\"', 'saying'): 1.0,\n",
       " ('saying', 'American'): 0.5,\n",
       " ('American', 'combat'): 0.5,\n",
       " ('combat', 'troops'): 1.0,\n",
       " ('troops', 'cannot'): 0.5,\n",
       " ('cannot', 'patrol'): 0.5,\n",
       " ('patrol', 'as'): 0.5,\n",
       " ('as', '<UNK>'): 57.0,\n",
       " ('<UNK>', 'as'): 66.0,\n",
       " ('as', 'they'): 18.0,\n",
       " ('they', 'did'): 6.5,\n",
       " ('did', 'before'): 0.5,\n",
       " ('before', 'pulling'): 0.5,\n",
       " ('pulling', 'out'): 1.0,\n",
       " ('out', 'of'): 55.5,\n",
       " ('of', 'cities'): 0.5,\n",
       " ('cities', 'on'): 0.5,\n",
       " ('on', 'June'): 3.0,\n",
       " ('June', '30'): 3.5,\n",
       " ('30', '.'): 2.0,\n",
       " ('.', '</s>'): 5702.5,\n",
       " ('</s>', '<s>'): 6052.0,\n",
       " ('<s>', 'Too'): 1.5,\n",
       " ('Too', 'soon'): 0.5,\n",
       " ('soon', 'to'): 0.5,\n",
       " ('to', 'tell'): 5.0,\n",
       " ('tell', 'but'): 0.5,\n",
       " ('but', 'I'): 14.0,\n",
       " ('I', \"'m\"): 33.0,\n",
       " (\"'m\", 'not'): 6.5,\n",
       " ('not', 'going'): 7.0,\n",
       " ('going', 'to'): 33.0,\n",
       " ('to', 'lose'): 2.5,\n",
       " ('lose', 'my'): 1.0,\n",
       " ('my', 'mind'): 1.5,\n",
       " ('mind', 'trying'): 0.5,\n",
       " ('trying', 'to'): 26.5,\n",
       " ('to', 'stay'): 9.0,\n",
       " ('stay', '2'): 0.5,\n",
       " ('2', 'steps'): 0.5,\n",
       " ('steps', 'ahead'): 0.5,\n",
       " ('ahead', 'of'): 16.0,\n",
       " ('of', 'what'): 12.0,\n",
       " ('what', 'will'): 1.0,\n",
       " ('will', 'eventually'): 1.0,\n",
       " ('eventually', 'reveal'): 0.5,\n",
       " ('reveal', 'itself'): 0.5,\n",
       " ('itself', 'without'): 0.5,\n",
       " ('without', 'any'): 2.0,\n",
       " ('any', 'effort'): 0.5,\n",
       " ('effort', 'at'): 0.5,\n",
       " ('at', 'all'): 10.5,\n",
       " ('all', '....'): 0.5,\n",
       " ('....', 'also'): 0.5,\n",
       " ('also', 'less'): 0.5,\n",
       " ('less', 'chance'): 0.5,\n",
       " ('chance', 'of'): 6.0,\n",
       " ('of', 'having'): 4.0,\n",
       " ('having', 'to'): 4.0,\n",
       " ('to', 'either'): 1.5,\n",
       " ('either', 'play'): 0.5,\n",
       " ('play', 'dumb'): 0.5,\n",
       " ('dumb', 'about'): 0.5,\n",
       " ('about', 'jumping'): 0.5,\n",
       " ('jumping', 'on'): 1.0,\n",
       " ('the', 'love'): 3.0,\n",
       " ('love', 'boat'): 0.5,\n",
       " ('boat', 'over'): 0.5,\n",
       " ('over', 'him'): 0.5,\n",
       " ('him', 'or'): 0.5,\n",
       " ('or', 'having'): 1.0,\n",
       " ('to', '<UNK>'): 357.0,\n",
       " ('<UNK>', 'mountains'): 1.0,\n",
       " ('mountains', 'of'): 1.0,\n",
       " ('of', '<UNK>'): 432.5,\n",
       " ('<UNK>', '.'): 870.5,\n",
       " ('<s>', '\"'): 400.5,\n",
       " ('\"', 'But'): 7.0,\n",
       " ('But', 'make'): 0.5,\n",
       " ('make', 'no'): 1.0,\n",
       " ('no', 'mistake'): 1.0,\n",
       " ('mistake', ':'): 0.5,\n",
       " (':', 'we'): 1.0,\n",
       " ('we', 'will'): 9.0,\n",
       " ('will', 'pursue'): 0.5,\n",
       " ('pursue', 'legal'): 0.5,\n",
       " ('legal', 'remedies'): 0.5,\n",
       " ('remedies', 'when'): 0.5,\n",
       " ('when', 'other'): 1.0,\n",
       " ('other', 'options'): 0.5,\n",
       " ('options', 'are'): 1.0,\n",
       " ('are', 'closed'): 0.5,\n",
       " ('closed', '.'): 2.0,\n",
       " ('<s>', '<UNK>'): 889.5,\n",
       " ('<UNK>', '<UNK>'): 1456.0,\n",
       " ('<UNK>', '('): 128.5,\n",
       " ('(', 'AP'): 53.5,\n",
       " ('AP', ')'): 54.0,\n",
       " (')', '-'): 67.5,\n",
       " ('-', 'Associated'): 0.5,\n",
       " ('Associated', 'Press'): 13.5,\n",
       " ('Press', 'Writer'): 2.0,\n",
       " ('Writer', 'WASHINGTON'): 0.5,\n",
       " ('WASHINGTON', ','): 3.5,\n",
       " (',', 'Iowa'): 1.0,\n",
       " ('Iowa', 'Fred'): 0.5,\n",
       " ('Fred', 'Thompson'): 1.0,\n",
       " ('Thompson', 'said'): 0.5,\n",
       " ('said', 'Saturday'): 1.5,\n",
       " ('Saturday', 'he'): 0.5,\n",
       " ('he', \"'s\"): 16.0,\n",
       " (\"'s\", 'not'): 17.5,\n",
       " ('not', '<UNK>'): 19.0,\n",
       " ('<UNK>', 'with'): 129.5,\n",
       " ('with', 'winning'): 0.5,\n",
       " ('winning', 'the'): 4.5,\n",
       " ('the', 'White'): 16.0,\n",
       " ('White', 'House'): 20.5,\n",
       " ('House', 'and'): 3.5,\n",
       " ('and', 'that'): 35.5,\n",
       " ('that', 'a'): 19.0,\n",
       " ('a', 'president'): 3.0,\n",
       " ('president', 'with'): 0.5,\n",
       " ('with', 'too'): 0.5,\n",
       " ('too', 'much'): 8.0,\n",
       " ('much', 'fire'): 0.5,\n",
       " ('fire', 'in'): 1.0,\n",
       " ('in', 'the'): 668.0,\n",
       " ('the', '<UNK>'): 1029.0,\n",
       " ('<UNK>', 'is'): 139.5,\n",
       " ('is', 'not'): 47.0,\n",
       " ('not', 'necessarily'): 2.5,\n",
       " ('necessarily', 'a'): 0.5,\n",
       " ('a', 'good'): 17.5,\n",
       " ('good', 'thing'): 0.5,\n",
       " ('thing', 'in'): 1.0,\n",
       " ('in', 'a'): 198.5,\n",
       " ('a', 'dangerous'): 1.0,\n",
       " ('dangerous', 'world'): 0.5,\n",
       " ('world', '.'): 15.0,\n",
       " ('<s>', 'The'): 881.0,\n",
       " ('The', 'higher'): 0.5,\n",
       " ('higher', 'rate'): 1.5,\n",
       " ('rate', 'of'): 5.5,\n",
       " ('of', 'Chinese'): 0.5,\n",
       " ('Chinese', 'college'): 0.5,\n",
       " ('college', '<UNK>'): 1.5,\n",
       " ('not', 'clearly'): 0.5,\n",
       " ('clearly', 'due'): 0.5,\n",
       " ('due', 'to'): 21.5,\n",
       " ('to', 'the'): 336.0,\n",
       " ('the', 'success'): 2.5,\n",
       " ('success', 'of'): 1.5,\n",
       " ('of', 'education'): 1.0,\n",
       " ('education', 'system'): 0.5,\n",
       " ('system', 'itself'): 0.5,\n",
       " ('itself', '.'): 4.0,\n",
       " ('<UNK>', 'to'): 272.5,\n",
       " ('to', '1,000'): 1.5,\n",
       " ('1,000', 'hours'): 1.0,\n",
       " ('hours', 'of'): 4.0,\n",
       " ('of', 'community'): 1.5,\n",
       " ('community', 'service'): 2.0,\n",
       " ('service', 'in'): 2.5,\n",
       " ('in', 'addition'): 3.0,\n",
       " ('addition', 'to'): 8.5,\n",
       " ('to', 'jail'): 4.0,\n",
       " ('jail', 'time'): 0.5,\n",
       " ('time', ','): 21.0,\n",
       " (',', 'he'): 96.0,\n",
       " ('he', 'has'): 26.5,\n",
       " ('has', 'spent'): 3.0,\n",
       " ('spent', 'much'): 1.5,\n",
       " ('much', 'of'): 12.0,\n",
       " ('of', 'that'): 14.0,\n",
       " ('that', 'time'): 3.5,\n",
       " ('time', 'touring'): 0.5,\n",
       " ('touring', 'the'): 0.5,\n",
       " ('the', 'country'): 44.0,\n",
       " ('country', 'giving'): 0.5,\n",
       " ('giving', 'talks'): 0.5,\n",
       " ('talks', 'to'): 2.5,\n",
       " ('to', 'children'): 0.5,\n",
       " ('children', 'in'): 2.0,\n",
       " ('in', '\"'): 8.0,\n",
       " ('\"', 'elementary'): 0.5,\n",
       " ('elementary', 'schools'): 1.5,\n",
       " ('schools', ','): 6.0,\n",
       " (',', 'middle'): 0.5,\n",
       " ('middle', 'schools'): 1.0,\n",
       " (',', 'high'): 1.5,\n",
       " ('high', 'schools'): 1.0,\n",
       " (',', 'juvenile'): 0.5,\n",
       " ('juvenile', 'centres'): 0.5,\n",
       " ('centres', ','): 1.0,\n",
       " (',', '<UNK>'): 643.5,\n",
       " ('<UNK>', ','): 1578.0,\n",
       " (',', 'clubs'): 0.5,\n",
       " ('clubs', ','): 1.5,\n",
       " (',', 'community'): 0.5,\n",
       " ('community', 'centres'): 0.5,\n",
       " ('centres', '...'): 0.5,\n",
       " ('...', 'everywhere'): 0.5,\n",
       " ('everywhere', ','): 1.0,\n",
       " (',', 'it'): 76.0,\n",
       " ('it', 'feels'): 1.0,\n",
       " ('feels', 'like'): 1.5,\n",
       " ('like', '.'): 1.0,\n",
       " ('.', '\"'): 121.5,\n",
       " ('\"', '</s>'): 117.5,\n",
       " ('<s>', 'In'): 172.5,\n",
       " ('In', '2007'): 1.0,\n",
       " ('2007', ','): 10.0,\n",
       " (',', 'another'): 7.5,\n",
       " ('another', '<UNK>'): 8.0,\n",
       " ('<UNK>', 'dressed'): 1.5,\n",
       " ('dressed', 'pair'): 0.5,\n",
       " ('pair', 'was'): 0.5,\n",
       " ('was', '<UNK>'): 89.0,\n",
       " ('the', 'store'): 4.0,\n",
       " ('store', \"'s\"): 0.5,\n",
       " (\"'s\", '<UNK>'): 209.5,\n",
       " ('<UNK>', 'Street'): 6.5,\n",
       " ('Street', 'branch'): 0.5,\n",
       " ('branch', 'in'): 0.5,\n",
       " ('a', 'Bentley'): 0.5,\n",
       " ('Bentley', 'Continental'): 0.5,\n",
       " ('Continental', '<UNK>'): 0.5,\n",
       " ('<s>', 'So'): 28.0,\n",
       " ('So', 'how'): 1.5,\n",
       " ('how', 'many'): 5.0,\n",
       " ('many', 'women'): 1.5,\n",
       " ('women', 'died'): 0.5,\n",
       " ('died', '?'): 0.5,\n",
       " ('?', '</s>'): 140.5,\n",
       " ('<s>', 'However'): 34.5,\n",
       " ('However', ','): 34.5,\n",
       " (',', 'the'): 474.5,\n",
       " ('<UNK>', 'of'): 375.0,\n",
       " ('of', 'a'): 160.5,\n",
       " ('a', '<UNK>'): 532.5,\n",
       " ('<UNK>', 'low'): 1.0,\n",
       " ('low', 'in'): 0.5,\n",
       " ('the', 'shares'): 2.5,\n",
       " ('shares', ','): 1.5,\n",
       " (',', 'which'): 191.5,\n",
       " ('which', 'at'): 1.5,\n",
       " ('at', '<UNK>'): 112.5,\n",
       " ('<UNK>', 'sit'): 1.0,\n",
       " ('sit', 'at'): 0.5,\n",
       " ('at', 'less'): 0.5,\n",
       " ('less', 'than'): 16.5,\n",
       " ('than', 'eight'): 1.5,\n",
       " ('eight', 'times'): 1.0,\n",
       " ('times', '<UNK>'): 1.0,\n",
       " ('<UNK>', 'earnings'): 1.0,\n",
       " ('earnings', 'and'): 1.0,\n",
       " ('and', 'yield'): 0.5,\n",
       " ('yield', '4'): 0.5,\n",
       " ('4', 'per'): 1.0,\n",
       " ('per', 'cent'): 50.0,\n",
       " ('cent', ','): 3.5,\n",
       " ('<UNK>', 'formidable'): 0.5,\n",
       " ('formidable', 'cyclical'): 0.5,\n",
       " ('cyclical', 'and'): 0.5,\n",
       " ('and', '<UNK>'): 506.0,\n",
       " ('<UNK>', 'pressures'): 0.5,\n",
       " ('pressures', 'and'): 0.5,\n",
       " ('and', '£'): 1.5,\n",
       " ('£', '1'): 1.5,\n",
       " ('1', 'billion'): 3.0,\n",
       " ('billion', 'of'): 1.5,\n",
       " ('of', 'debt'): 2.5,\n",
       " ('debt', '.'): 4.0,\n",
       " ('<s>', 'But'): 189.5,\n",
       " ('But', 'we'): 3.0,\n",
       " ('we', 'believe'): 3.5,\n",
       " ('believe', 'that'): 9.5,\n",
       " ('that', 'the'): 156.5,\n",
       " ('the', 'United'): 52.0,\n",
       " ('United', 'States'): 50.0,\n",
       " ('States', 'is'): 3.0,\n",
       " ('is', 'a'): 101.5,\n",
       " ('a', 'credible'): 1.0,\n",
       " ('credible', 'country'): 0.5,\n",
       " ('country', 'and'): 2.5,\n",
       " ('and', 'particularly'): 0.5,\n",
       " ('particularly', 'at'): 0.5,\n",
       " ('at', 'such'): 1.0,\n",
       " ('such', 'difficult'): 0.5,\n",
       " ('difficult', 'times'): 0.5,\n",
       " ('times', ','): 3.0,\n",
       " (',', 'China'): 7.0,\n",
       " ('China', 'has'): 4.0,\n",
       " ('has', 'reached'): 0.5,\n",
       " ('reached', 'out'): 1.0,\n",
       " ('out', 'to'): 8.5,\n",
       " ('States', '.'): 9.0,\n",
       " ('<s>', 'Certainly'): 1.5,\n",
       " ('Certainly', 'for'): 0.5,\n",
       " ('for', 'the'): 269.0,\n",
       " ('the', 'last'): 33.0,\n",
       " ('last', 'six'): 1.0,\n",
       " ('six', 'months'): 9.0,\n",
       " ('months', 'I'): 0.5,\n",
       " ('I', 'had'): 12.5,\n",
       " ('had', 'a'): 33.5,\n",
       " ('a', 'bit'): 6.0,\n",
       " ('bit', 'of'): 4.5,\n",
       " ('a', 'mask'): 1.0,\n",
       " ('mask', 'on'): 0.5,\n",
       " ('on', 'and'): 3.5,\n",
       " ('and', 'it'): 27.0,\n",
       " ('it', 'came'): 2.0,\n",
       " ('came', 'down'): 1.0,\n",
       " ('down', 'during'): 0.5,\n",
       " ('during', 'that'): 2.0,\n",
       " ('that', 'press'): 0.5,\n",
       " ('press', 'conference'): 5.5,\n",
       " ('conference', '.'): 3.5,\n",
       " ('<s>', 'Yet'): 8.0,\n",
       " ('Yet', 'the'): 1.5,\n",
       " ('the', 'two'): 19.0,\n",
       " ('two', 'are'): 1.0,\n",
       " ('are', 'not'): 29.5,\n",
       " ('not', 'close'): 0.5,\n",
       " ('close', '.'): 3.5,\n",
       " ('<s>', 'Barger'): 0.5,\n",
       " ('Barger', 'wants'): 0.5,\n",
       " ('wants', 'the'): 1.5,\n",
       " ('the', 'court'): 6.5,\n",
       " ('court', 'to'): 0.5,\n",
       " ('to', 'declare'): 1.0,\n",
       " ('declare', 'the'): 0.5,\n",
       " ('the', '\"'): 36.0,\n",
       " ('\"', '1'): 0.5,\n",
       " ('1', '%'): 2.5,\n",
       " ('%', '<UNK>'): 4.0,\n",
       " ('<UNK>', '\"'): 126.0,\n",
       " ('\"', 'as'): 6.0,\n",
       " ('as', 'a'): 112.0,\n",
       " ('a', 'joint'): 4.0,\n",
       " ('joint', 'work'): 0.5,\n",
       " ('work', 'of'): 3.5,\n",
       " ('<UNK>', 'Barger'): 0.5,\n",
       " ('Barger', '<UNK>'): 0.5,\n",
       " ('<UNK>', 'and'): 548.0,\n",
       " ('<UNK>', 'from'): 89.5,\n",
       " ('from', 'selling'): 0.5,\n",
       " ('selling', 'or'): 0.5,\n",
       " ('or', '<UNK>'): 60.0,\n",
       " ('<UNK>', 'the'): 234.0,\n",
       " ('the', 'program'): 6.0,\n",
       " ('program', ','): 7.0,\n",
       " (',', 'and'): 378.0,\n",
       " ('and', 'award'): 0.5,\n",
       " ('award', '<UNK>'): 0.5,\n",
       " ('<UNK>', 'damages'): 0.5,\n",
       " ('damages', 'for'): 0.5,\n",
       " ('for', 'exploiting'): 0.5,\n",
       " ('exploiting', 'Barger'): 0.5,\n",
       " ('Barger', \"'s\"): 0.5,\n",
       " (\"'s\", 'publicity'): 0.5,\n",
       " ('publicity', 'rights'): 0.5,\n",
       " ('rights', '.'): 4.5,\n",
       " ('<s>', 'A'): 146.5,\n",
       " ('A', '<UNK>'): 27.0,\n",
       " ('<UNK>', 'Boeing'): 0.5,\n",
       " ('Boeing', '<UNK>'): 1.0,\n",
       " ('<UNK>', 'crashed'): 2.0,\n",
       " ('crashed', 'last'): 0.5,\n",
       " ('last', 'year'): 53.5,\n",
       " ('year', 'in'): 4.5,\n",
       " ('the', 'northern'): 1.0,\n",
       " ('northern', 'city'): 1.0,\n",
       " ('city', 'of'): 8.0,\n",
       " ('of', 'M'): 0.5,\n",
       " ('M', '<UNK>'): 1.5,\n",
       " ('<UNK>', 'Congo'): 0.5,\n",
       " ('Congo', ','): 0.5,\n",
       " (',', 'killing'): 6.5,\n",
       " ('killing', 'a'): 1.5,\n",
       " ('a', 'handful'): 3.5,\n",
       " ('handful', 'of'): 3.5,\n",
       " ('of', 'passengers'): 0.5,\n",
       " ('passengers', 'and'): 2.0,\n",
       " ('and', 'injuring'): 0.5,\n",
       " ('injuring', 'dozens'): 0.5,\n",
       " ('dozens', 'of'): 6.0,\n",
       " ('of', 'others'): 1.5,\n",
       " ('others', '.'): 3.5,\n",
       " ('<s>', 'We'): 45.0,\n",
       " ('We', '<UNK>'): 2.5,\n",
       " ('<UNK>', 'need'): 1.0,\n",
       " ('need', 'a'): 6.5,\n",
       " ('a', 'penny'): 0.5,\n",
       " ('penny', 'for'): 0.5,\n",
       " ('the', 'thoughts'): 0.5,\n",
       " ('thoughts', 'of'): 0.5,\n",
       " ('of', 'Sir'): 1.5,\n",
       " ('Sir', 'Alex'): 2.0,\n",
       " ('Alex', 'Ferguson'): 2.0,\n",
       " ('Ferguson', ','): 1.5,\n",
       " (',', 'or'): 57.5,\n",
       " ('or', 'the'): 15.0,\n",
       " ('<UNK>', 'Martin'): 1.0,\n",
       " ('Martin', 'O'): 0.5,\n",
       " ('O', \"'Neill\"): 3.5,\n",
       " (\"'Neill\", ','): 1.5,\n",
       " (',', 'in'): 80.0,\n",
       " ('the', 'wake'): 3.0,\n",
       " ('wake', 'of'): 3.0,\n",
       " ('of', 'Scolari'): 0.5,\n",
       " ('Scolari', \"'s\"): 0.5,\n",
       " ('The', 'UK'): 2.0,\n",
       " ('UK', 'capital'): 0.5,\n",
       " ('capital', 'took'): 0.5,\n",
       " ('took', 'the'): 7.5,\n",
       " ('<UNK>', 'by'): 101.5,\n",
       " ('by', 'a'): 42.0,\n",
       " ('a', 'huge'): 6.0,\n",
       " ('huge', 'margin'): 0.5,\n",
       " ('margin', ','): 1.5,\n",
       " (',', 'with'): 84.5,\n",
       " ('with', '36'): 0.5,\n",
       " ('36', 'per'): 1.0,\n",
       " ('cent', '<UNK>'): 0.5,\n",
       " ('<UNK>', 'London'): 3.0,\n",
       " ('London', 'was'): 0.5,\n",
       " ('was', 'the'): 43.0,\n",
       " ('<UNK>', '--'): 51.0,\n",
       " ('--', 'well'): 0.5,\n",
       " ('well', 'ahead'): 1.0,\n",
       " ('of', 'Paris'): 1.5,\n",
       " ('Paris', 'in'): 1.0,\n",
       " ('in', 'second'): 1.5,\n",
       " ('second', 'place'): 2.0,\n",
       " ('place', 'with'): 2.5,\n",
       " ('with', '9'): 0.5,\n",
       " ('9', 'per'): 1.5,\n",
       " ('cent', '.'): 7.5,\n",
       " ('A', '15-year-old'): 0.5,\n",
       " ('15-year-old', 'boy'): 1.0,\n",
       " ('boy', 'from'): 0.5,\n",
       " ('from', '<UNK>'): 89.0,\n",
       " ('<UNK>', 'who'): 31.0,\n",
       " ('who', 'has'): 23.0,\n",
       " ('has', 'been'): 111.5,\n",
       " ('been', 'missing'): 0.5,\n",
       " ('missing', 'for'): 0.5,\n",
       " ('for', 'a'): 109.0,\n",
       " ('a', 'week'): 9.0,\n",
       " ('week', 'has'): 2.0,\n",
       " ('been', 'found'): 4.0,\n",
       " ('found', 'after'): 0.5,\n",
       " ('after', 'a'): 34.5,\n",
       " ('a', 'police'): 6.0,\n",
       " ('police', 'appeal'): 0.5,\n",
       " ('appeal', '.'): 1.5,\n",
       " ('<s>', 'It'): 186.5,\n",
       " ('It', 'supports'): 0.5,\n",
       " ('supports', 'a'): 0.5,\n",
       " ('<UNK>', 'pen'): 0.5,\n",
       " ('pen', ','): 1.0,\n",
       " (',', 'but'): 210.0,\n",
       " ('but', 'also'): 7.0,\n",
       " ('also', '<UNK>'): 20.5,\n",
       " ('with', 'your'): 3.5,\n",
       " ('your', 'finger'): 0.5,\n",
       " ('finger', '.'): 0.5,\n",
       " ('<s>', 'And'): 82.0,\n",
       " ('And', 'in'): 3.5,\n",
       " ('in', 'April'): 7.5,\n",
       " ('April', 'of'): 0.5,\n",
       " ('of', 'this'): 29.5,\n",
       " ('this', 'year'): 51.0,\n",
       " ('year', 'he'): 1.0,\n",
       " ('he', 'sat'): 0.5,\n",
       " ('sat', 'in'): 1.0,\n",
       " ('a', 'Tokyo'): 0.5,\n",
       " ('Tokyo', 'courtroom'): 0.5,\n",
       " ('courtroom', 'to'): 0.5,\n",
       " ('to', 'hear'): 6.5,\n",
       " ('hear', 'a'): 0.5,\n",
       " ('<UNK>', 'businessman'): 0.5,\n",
       " ('businessman', ','): 2.0,\n",
       " ('and', 'killing'): 1.5,\n",
       " ('killing', 'his'): 1.0,\n",
       " ('his', 'daughter'): 2.0,\n",
       " ('daughter', ','): 5.0,\n",
       " (',', 'although'): 12.5,\n",
       " ('although', 'he'): 2.0,\n",
       " ('he', 'was'): 81.5,\n",
       " ('was', 'sentenced'): 1.0,\n",
       " ('sentenced', 'effectively'): 0.5,\n",
       " ('effectively', 'to'): 0.5,\n",
       " ('to', 'life'): 2.5,\n",
       " ('life', 'imprisonment'): 1.0,\n",
       " ('imprisonment', 'for'): 0.5,\n",
       " ('for', 'nine'): 1.0,\n",
       " ('nine', 'other'): 0.5,\n",
       " ('other', 'rapes'): 0.5,\n",
       " ('rapes', ','): 1.0,\n",
       " (',', 'including'): 64.5,\n",
       " ('including', 'the'): 16.0,\n",
       " ('the', 'rape'): 1.0,\n",
       " ('rape', 'and'): 0.5,\n",
       " ('and', 'murder'): 0.5,\n",
       " ('murder', 'of'): 3.0,\n",
       " ('of', 'an'): 26.5,\n",
       " ('an', 'Australian'): 1.0,\n",
       " ('Australian', '<UNK>'): 1.5,\n",
       " ('in', 'February'): 7.0,\n",
       " ('February', '1992'): 0.5,\n",
       " ('1992', '.'): 2.5,\n",
       " ('<s>', 'One'): 32.5,\n",
       " ('One', 'of'): 9.0,\n",
       " ('of', 'the'): 794.5,\n",
       " ('the', 'houses'): 1.5,\n",
       " ('houses', 'that'): 0.5,\n",
       " ('that', 'we'): 17.5,\n",
       " ('we', 'did'): 2.0,\n",
       " ('did', ','): 2.5,\n",
       " ('the', 'dogs'): 1.5,\n",
       " ('dogs', ','): 1.5,\n",
       " (',', 'I'): 48.0,\n",
       " ('I', 'guess'): 3.0,\n",
       " ('guess', ','): 0.5,\n",
       " (',', 'loved'): 0.5,\n",
       " ('loved', 'music'): 0.5,\n",
       " ('music', ','): 4.0,\n",
       " (',', 'according'): 50.0,\n",
       " ('according', 'to'): 58.0,\n",
       " ('the', 'client'): 0.5,\n",
       " ('client', ','): 0.5,\n",
       " (',', 'so'): 22.0,\n",
       " ('so', 'they'): 5.5,\n",
       " ('they', 'had'): 11.0,\n",
       " ('a', 'stereo'): 0.5,\n",
       " ('stereo', 'system'): 0.5,\n",
       " ('system', 'put'): 0.5,\n",
       " ('put', 'in'): 3.0,\n",
       " ('in', '.'): 6.5,\n",
       " (',', 'Hannah'): 0.5,\n",
       " ('Hannah', 'told'): 0.5,\n",
       " ('told', '<UNK>'): 7.0,\n",
       " ('<UNK>', 'that'): 96.5,\n",
       " ('that', 'she'): 18.5,\n",
       " ('she', 'hopes'): 1.0,\n",
       " ('hopes', 'her'): 0.5,\n",
       " ('her', 'experience'): 1.0,\n",
       " ('experience', 'will'): 0.5,\n",
       " ('will', 'help'): 6.5,\n",
       " ('help', 'other'): 1.0,\n",
       " ('other', 'children'): 1.5,\n",
       " ('children', 'like'): 0.5,\n",
       " ('like', 'her'): 1.5,\n",
       " ('her', 'keep'): 0.5,\n",
       " ('keep', 'open'): 0.5,\n",
       " ('open', 'lines'): 0.5,\n",
       " ('lines', 'of'): 1.5,\n",
       " ('of', 'communication'): 0.5,\n",
       " ('communication', 'with'): 1.0,\n",
       " ('with', 'their'): 7.0,\n",
       " ('their', 'parents'): 2.0,\n",
       " ('parents', 'when'): 0.5,\n",
       " ('when', 'it'): 18.0,\n",
       " ('it', 'comes'): 5.0,\n",
       " ('comes', 'to'): 6.0,\n",
       " ('to', 'health'): 0.5,\n",
       " ('health', 'issues'): 1.5,\n",
       " ('issues', '.'): 4.0,\n",
       " ('The', 'spokesperson'): 0.5,\n",
       " ('spokesperson', 'said'): 1.5,\n",
       " ('said', 'a'): 6.5,\n",
       " ('a', 'fresh'): 4.5,\n",
       " ('fresh', 'offshore'): 0.5,\n",
       " ('offshore', '<UNK>'): 0.5,\n",
       " ('<UNK>', 'was'): 103.0,\n",
       " ('and', 'the'): 200.0,\n",
       " ('<UNK>', 'were'): 33.5,\n",
       " ('were', '<UNK>'): 34.5,\n",
       " ('<UNK>', 'quite'): 0.5,\n",
       " ('quite', 'rapidly'): 0.5,\n",
       " ('rapidly', 'when'): 0.5,\n",
       " ('when', 'they'): 14.0,\n",
       " ('they', 'were'): 34.0,\n",
       " ('were', 'spotted'): 1.0,\n",
       " ('spotted', 'at'): 0.5,\n",
       " ('at', 'about'): 5.5,\n",
       " ('about', '2030'): 0.5,\n",
       " ('2030', 'BST'): 0.5,\n",
       " ('BST', 'on'): 5.5,\n",
       " ('on', 'Tuesday'): 34.0,\n",
       " ('Tuesday', '.'): 16.5,\n",
       " ('<UNK>', 'FOR'): 1.0,\n",
       " ('FOR', '<UNK>'): 0.5,\n",
       " ('<s>', 'As'): 52.0,\n",
       " ('As', 'an'): 0.5,\n",
       " ('an', 'alternative'): 3.5,\n",
       " ('alternative', ','): 0.5,\n",
       " ('the', 'housing'): 5.5,\n",
       " ('housing', 'authority'): 0.5,\n",
       " ('authority', 'is'): 0.5,\n",
       " ('is', 'considering'): 3.0,\n",
       " ('considering', 'reducing'): 0.5,\n",
       " ('reducing', 'the'): 1.0,\n",
       " ('the', 'value'): 4.0,\n",
       " ('value', 'of'): 4.5,\n",
       " ('of', 'each'): 1.5,\n",
       " ('each', '<UNK>'): 1.0,\n",
       " (',', 'meaning'): 2.5,\n",
       " ('meaning', 'that'): 0.5,\n",
       " ('that', '<UNK>'): 86.5,\n",
       " (',', 'tenants'): 0.5,\n",
       " ('tenants', 'or'): 0.5,\n",
       " ('or', 'both'): 0.5,\n",
       " ('both', 'would'): 0.5,\n",
       " ('would', 'have'): 23.0,\n",
       " ('have', 'to'): 50.0,\n",
       " ('the', 'cost'): 8.0,\n",
       " ('cost', '.'): 2.0,\n",
       " ('One', 'novel'): 0.5,\n",
       " ('novel', ','): 0.5,\n",
       " ('\"', '<UNK>'): 252.0,\n",
       " ('\"', 'was'): 6.5,\n",
       " ('was', 'a'): 69.0,\n",
       " ('a', 'best'): 1.0,\n",
       " ('best', 'seller'): 0.5,\n",
       " ('seller', '.'): 0.5,\n",
       " ('<s>', 'He'): 173.0,\n",
       " ('He', 'is'): 9.0,\n",
       " ('is', 'serving'): 1.0,\n",
       " ('serving', 'a'): 1.5,\n",
       " ('a', 'minimum'): 0.5,\n",
       " ('minimum', 'of'): 1.0,\n",
       " ('of', '27'): 0.5,\n",
       " ('27', 'years'): 0.5,\n",
       " ('years', '.'): 31.5,\n",
       " ('<s>', 'She'): 41.5,\n",
       " ('She', 'said'): 8.0,\n",
       " ('said', 'she'): 13.0,\n",
       " ('she', 'had'): 11.5,\n",
       " ('had', 'acted'): 1.0,\n",
       " ('acted', 'within'): 0.5,\n",
       " ('within', 'the'): 10.0,\n",
       " ('the', 'rules'): 3.0,\n",
       " ('rules', 'of'): 1.5,\n",
       " ('the', 'Commons'): 1.5,\n",
       " ('Commons', 'and'): 0.5,\n",
       " ('<UNK>', 'Revenue'): 1.0,\n",
       " ('Revenue', ','): 0.5,\n",
       " ('but', 'told'): 0.5,\n",
       " ('told', 'Sky'): 0.5,\n",
       " ('Sky', 'News'): 0.5,\n",
       " ('News', 'she'): 0.5,\n",
       " ('had', 'decided'): 2.5,\n",
       " ('decided', 'to'): 7.0,\n",
       " ('to', 'pay'): 15.0,\n",
       " ('pay', 'the'): 2.0,\n",
       " ('the', 'money'): 8.0,\n",
       " ('money', 'anyway'): 0.5,\n",
       " ('anyway', 'after'): 0.5,\n",
       " ('after', 'public'): 0.5,\n",
       " ('public', '<UNK>'): 3.5,\n",
       " ('\"', 'Jews'): 0.5,\n",
       " ('Jews', 'were'): 0.5,\n",
       " ('were', 'certainly'): 0.5,\n",
       " ('certainly', 'an'): 0.5,\n",
       " ('an', '<UNK>'): 93.5,\n",
       " ('<UNK>', 'people'): 17.0,\n",
       " ('people', '.'): 11.5,\n",
       " ('<UNK>', \"'s\"): 242.5,\n",
       " ('<UNK>', 'a'): 93.5,\n",
       " ('a', 'penalty'): 2.5,\n",
       " ('penalty', 'kick'): 1.0,\n",
       " ('kick', 'in'): 1.0,\n",
       " ('<UNK>', 'minute'): 2.0,\n",
       " ('minute', 'and'): 1.0,\n",
       " ('and', 'had'): 12.0,\n",
       " ('a', 'chance'): 7.0,\n",
       " ('chance', 'to'): 10.5,\n",
       " ('to', 'tie'): 1.0,\n",
       " ('tie', 'it'): 0.5,\n",
       " ('it', 'in'): 9.0,\n",
       " ('but', 'his'): 3.0,\n",
       " ('his', 'diving'): 0.5,\n",
       " ('diving', '<UNK>'): 0.5,\n",
       " ('<UNK>', 'went'): 5.5,\n",
       " ('went', 'just'): 0.5,\n",
       " ('just', 'wide'): 0.5,\n",
       " ('wide', 'of'): 1.5,\n",
       " ('the', 'right'): 16.0,\n",
       " ('right', 'post'): 0.5,\n",
       " ('post', '.'): 1.0,\n",
       " ('<s>', 'For'): 37.0,\n",
       " ('For', '<UNK>'): 3.0,\n",
       " ('<UNK>', 'Carter'): 1.0,\n",
       " ('Carter', ','): 1.0,\n",
       " (',', 'who'): 178.5,\n",
       " ('who', 'made'): 4.5,\n",
       " ('made', 'her'): 2.0,\n",
       " ('her', 'New'): 1.0,\n",
       " ('New', 'York'): 66.0,\n",
       " ('York', '<UNK>'): 5.0,\n",
       " ('<UNK>', 'debut'): 1.5,\n",
       " ('debut', 'on'): 0.5,\n",
       " ('Tuesday', 'evening'): 1.5,\n",
       " ('evening', 'at'): 1.0,\n",
       " (\"'s\", 'at'): 1.5,\n",
       " ('<UNK>', 'Regency'): 1.0,\n",
       " ('Regency', ','): 0.5,\n",
       " (',', 'that'): 34.5,\n",
       " ('that', 'meant'): 1.0,\n",
       " ('meant', 'taking'): 0.5,\n",
       " ('taking', 'on'): 1.0,\n",
       " ('on', '<UNK>'): 60.0,\n",
       " ('\"', 'Blues'): 0.5,\n",
       " ('Blues', 'in'): 0.5,\n",
       " ('the', 'Night'): 0.5,\n",
       " ('Night', '\"'): 1.0,\n",
       " ('\"', 'and'): 37.5,\n",
       " ('<UNK>', 'it'): 27.0,\n",
       " ('it', 'to'): 16.0,\n",
       " ('the', 'ground'): 10.0,\n",
       " ('ground', '.'): 5.0,\n",
       " ('<s>', 'You'): 19.5,\n",
       " ('You', 'will'): 2.0,\n",
       " ('will', 'still'): 1.5,\n",
       " ('still', 'have'): 3.0,\n",
       " ('have', 'the'): 16.5,\n",
       " ('<UNK>', 'pension'): 1.5,\n",
       " ('pension', 'you'): 0.5,\n",
       " ('you', 'have'): 12.0,\n",
       " ('have', 'built'): 0.5,\n",
       " ('built', 'up'): 2.5,\n",
       " ('up', ','): 7.0,\n",
       " ('but', 'no'): 5.5,\n",
       " ('no', 'more'): 5.5,\n",
       " ('more', 'pension'): 0.5,\n",
       " ('pension', 'will'): 0.5,\n",
       " ('will', 'be'): 109.5,\n",
       " ('be', 'added'): 1.5,\n",
       " ('added', '.'): 9.5,\n",
       " ('But', 'Napolitano'): 0.5,\n",
       " ('Napolitano', 'said'): 1.5,\n",
       " ('said', 'the'): 104.0,\n",
       " ('U.S.', 'has'): 1.5,\n",
       " ('has', 'no'): 5.5,\n",
       " ('no', 'plans'): 1.5,\n",
       " ('plans', 'to'): 21.5,\n",
       " ('to', 'close'): 7.5,\n",
       " ('close', 'the'): 1.0,\n",
       " ('the', 'border'): 6.0,\n",
       " ('border', 'with'): 2.0,\n",
       " ('with', 'Mexico'): 0.5,\n",
       " ('Mexico', ','): 4.5,\n",
       " (',', 'a'): 210.5,\n",
       " ('a', 'stance'): 0.5,\n",
       " ('stance', 'that'): 1.0,\n",
       " ('the', 'Centers'): 1.0,\n",
       " ('Centers', 'for'): 0.5,\n",
       " ('for', 'Disease'): 0.5,\n",
       " ('Disease', '<UNK>'): 1.0,\n",
       " ('(', 'CDC'): 0.5,\n",
       " ('CDC', ')'): 0.5,\n",
       " (')', 'and'): 25.0,\n",
       " ('the', 'WHO'): 1.0,\n",
       " ('WHO', 'support'): 0.5,\n",
       " ('support', 'and'): 2.0,\n",
       " ('and', 'which'): 2.5,\n",
       " ('which', 'Obama'): 0.5,\n",
       " ('Obama', 'reiterated'): 0.5,\n",
       " ('reiterated', 'at'): 0.5,\n",
       " ('at', 'his'): 12.0,\n",
       " ('his', 'press'): 0.5,\n",
       " ('<UNK>', 'emerged'): 1.5,\n",
       " ('emerged', 'only'): 0.5,\n",
       " ('only', 'at'): 1.5,\n",
       " ('at', 'night'): 2.5,\n",
       " ('night', 'to'): 2.0,\n",
       " ('to', 'feed'): 3.0,\n",
       " ('feed', 'under'): 0.5,\n",
       " ('under', 'cover'): 0.5,\n",
       " ('cover', 'of'): 0.5,\n",
       " ('of', 'darkness'): 0.5,\n",
       " ('darkness', 'and'): 0.5,\n",
       " ('<UNK>', '60'): 1.5,\n",
       " ('60', 'miles'): 1.5,\n",
       " ('miles', 'from'): 3.0,\n",
       " ('from', 'her'): 6.0,\n",
       " ('her', 'home'): 3.0,\n",
       " ('home', 'field'): 0.5,\n",
       " ('field', 'to'): 0.5,\n",
       " ('to', 'avoid'): 13.0,\n",
       " ('avoid', 'detection'): 1.0,\n",
       " ('detection', '.'): 1.0,\n",
       " ('<s>', 'Because'): 5.5,\n",
       " ('Because', 'they'): 0.5,\n",
       " ('they', 'are'): 42.0,\n",
       " ('are', 'placed'): 1.0,\n",
       " ('placed', 'in'): 3.0,\n",
       " ('in', 'an'): 34.5,\n",
       " ('<UNK>', 'account'): 1.5,\n",
       " ('account', ','): 1.0,\n",
       " (',', 'funds'): 0.5,\n",
       " ('funds', 'should'): 0.5,\n",
       " ('should', 'always'): 0.5,\n",
       " ('always', 'be'): 1.5,\n",
       " ('be', 'available'): 5.0,\n",
       " ('available', 'to'): 3.5,\n",
       " ('to', 'make'): 48.5,\n",
       " ('make', 'the'): 11.5,\n",
       " ('the', 'payments'): 0.5,\n",
       " ('payments', '.'): 4.0,\n",
       " ('the', 'UK'): 21.0,\n",
       " ('UK', 'justice'): 0.5,\n",
       " ('justice', 'department'): 0.5,\n",
       " ('department', 'said'): 1.5,\n",
       " ('said', 'any'): 1.0,\n",
       " ('any', 'decision'): 0.5,\n",
       " ('decision', 'on'): 2.5,\n",
       " ('on', 'transfer'): 0.5,\n",
       " ('transfer', 'was'): 0.5,\n",
       " ('a', 'matter'): 2.0,\n",
       " ('matter', 'for'): 0.5,\n",
       " ('the', 'Scottish'): 6.0,\n",
       " ('Scottish', 'courts'): 0.5,\n",
       " ('courts', 'and'): 0.5,\n",
       " ('and', 'could'): 5.0,\n",
       " ('could', 'not'): 20.0,\n",
       " ('not', 'go'): 1.0,\n",
       " ('go', 'ahead'): 2.0,\n",
       " ('ahead', 'without'): 0.5,\n",
       " ('without', 'the'): 2.5,\n",
       " ('the', 'approval'): 1.5,\n",
       " ('approval', 'of'): 2.0,\n",
       " ('Scottish', 'Government'): 2.5,\n",
       " ('Government', '.'): 1.5,\n",
       " ('<s>', 'Mr'): 39.5,\n",
       " ('Mr', '<UNK>'): 52.0,\n",
       " ('was', 'first'): 6.5,\n",
       " ('first', 'arrested'): 0.5,\n",
       " ('arrested', 'over'): 0.5,\n",
       " ('over', 'her'): 1.5,\n",
       " ('her', 'death'): 2.0,\n",
       " ('death', 'five'): 0.5,\n",
       " ('five', 'days'): 5.0,\n",
       " ('days', 'after'): 6.5,\n",
       " ('after', 'the'): 61.0,\n",
       " ('the', 'discovery'): 2.0,\n",
       " ('discovery', ','): 1.5,\n",
       " ('and', 'was'): 19.5,\n",
       " ('<UNK>', 'on'): 129.0,\n",
       " ('on', '12'): 1.5,\n",
       " ('12', 'February'): 0.5,\n",
       " ('February', '.'): 3.5,\n",
       " ('<s>', 'Overall'): 2.5,\n",
       " ('Overall', ','): 3.0,\n",
       " (',', 'voters'): 1.0,\n",
       " ('voters', 'say'): 0.5,\n",
       " ('say', 'that'): 11.5,\n",
       " ('that', '\"'): 8.5,\n",
       " ('\"', 'strong'): 1.5,\n",
       " ('strong', 'leadership'): 0.5,\n",
       " ('leadership', 'skills'): 1.0,\n",
       " ('skills', '\"'): 0.5,\n",
       " ('\"', 'are'): 3.5,\n",
       " ('are', 'more'): 6.5,\n",
       " ('more', 'important'): 2.0,\n",
       " ('important', 'in'): 0.5,\n",
       " ('president', 'than'): 0.5,\n",
       " ('than', '\"'): 0.5,\n",
       " ('strong', 'moral'): 0.5,\n",
       " ('moral', 'character'): 0.5,\n",
       " ('character', '\"'): 0.5,\n",
       " ('\"', 'by'): 9.5,\n",
       " ('by', '10'): 1.0,\n",
       " ('10', 'percentage'): 1.0,\n",
       " ('percentage', 'points'): 3.0,\n",
       " ('points', ','): 8.5,\n",
       " ('with', 'another'): 3.0,\n",
       " ('another', '32'): 0.5,\n",
       " ('32', 'percent'): 1.5,\n",
       " ('percent', 'unable'): 0.5,\n",
       " ('unable', 'to'): 9.0,\n",
       " ('to', 'choose'): 2.5,\n",
       " ('choose', 'one'): 0.5,\n",
       " ('one', 'over'): 0.5,\n",
       " ('over', 'the'): 71.0,\n",
       " ('the', 'other'): 24.5,\n",
       " ('other', '.'): 3.5,\n",
       " ('<s>', 'Many'): 15.5,\n",
       " ('Many', 'higher'): 0.5,\n",
       " ('higher', 'risk'): 2.5,\n",
       " ('risk', 'surgical'): 0.5,\n",
       " ('surgical', 'patients'): 0.5,\n",
       " ('patients', 'and'): 2.0,\n",
       " ('<UNK>', 'patients'): 1.0,\n",
       " ('patients', 'continue'): 0.5,\n",
       " ('continue', 'to'): 14.5,\n",
       " ('to', 'be'): 184.0,\n",
       " ('be', 'affected'): 2.0,\n",
       " ('affected', 'by'): 3.5,\n",
       " ('by', 'the'): 148.5,\n",
       " ('the', 'chronic'): 1.0,\n",
       " ('chronic', 'volume'): 0.5,\n",
       " ('volume', '<UNK>'): 0.5,\n",
       " ('<UNK>', 'caused'): 3.5,\n",
       " ('caused', 'by'): 6.0,\n",
       " ('by', '<UNK>'): 84.0,\n",
       " ('which', 'requires'): 1.0,\n",
       " ('requires', 'the'): 0.5,\n",
       " ('the', 'heart'): 5.5,\n",
       " ('heart', 'to'): 0.5,\n",
       " ('to', 'work'): 21.5,\n",
       " ('work', 'harder'): 1.0,\n",
       " ('harder', ','): 1.5,\n",
       " ('and', 'may'): 4.0,\n",
       " ('may', 'ultimately'): 0.5,\n",
       " ('ultimately', 'lead'): 0.5,\n",
       " ('lead', 'to'): 7.5,\n",
       " ('to', 'heart'): 1.0,\n",
       " ('heart', 'failure'): 0.5,\n",
       " ('failure', '.'): 2.0,\n",
       " ('<UNK>', 'drugs'): 2.0,\n",
       " ('drugs', 'and'): 2.0,\n",
       " ('and', 'destroy'): 0.5,\n",
       " ('destroy', 'the'): 0.5,\n",
       " ('the', 'networks'): 1.5,\n",
       " ('networks', 'that'): 0.5,\n",
       " ('that', 'recruit'): 0.5,\n",
       " ('recruit', 'otherwise'): 0.5,\n",
       " ('otherwise', 'law'): 0.5,\n",
       " ('law', '<UNK>'): 1.5,\n",
       " ('<UNK>', 'citizens'): 1.5,\n",
       " ('citizens', 'into'): 0.5,\n",
       " ('into', '<UNK>'): 11.5,\n",
       " ('<UNK>', 'crimes'): 1.0,\n",
       " ('crimes', ','): 2.0,\n",
       " (',', 'ID'): 0.5,\n",
       " ('ID', 'theft'): 0.5,\n",
       " ('theft', 'and'): 0.5,\n",
       " ('and', 'other'): 44.5,\n",
       " ('other', 'criminal'): 0.5,\n",
       " ('criminal', 'activity'): 1.0,\n",
       " ('activity', '.'): 1.0,\n",
       " ('from', 'all'): 3.5,\n",
       " ('all', 'that'): 4.0,\n",
       " ('that', ','): 21.5,\n",
       " ('the', 'sun'): 3.5,\n",
       " ('sun', 'is'): 0.5,\n",
       " ('is', '<UNK>'): 87.0,\n",
       " ('<UNK>', 'brightly'): 0.5,\n",
       " ('brightly', 'beside'): 0.5,\n",
       " ('beside', 'the'): 1.5,\n",
       " ('the', 'Mediterranean'): 1.5,\n",
       " ('Mediterranean', '.'): 0.5,\n",
       " ('The', 'committee'): 2.5,\n",
       " ('committee', 'members'): 0.5,\n",
       " ('members', 'discussed'): 0.5,\n",
       " ('discussed', 'financial'): 0.5,\n",
       " ('financial', 'and'): 2.5,\n",
       " ('and', 'economic'): 3.5,\n",
       " ('economic', 'topics'): 0.5,\n",
       " ('topics', 'on'): 0.5,\n",
       " ('the', 'GCC'): 0.5,\n",
       " ('GCC', 'summit'): 0.5,\n",
       " ('summit', 'agenda'): 0.5,\n",
       " ('agenda', ','): 1.5,\n",
       " ('including', 'a'): 9.0,\n",
       " ('a', 'report'): 5.0,\n",
       " ('report', 'on'): 2.5,\n",
       " ('on', 'customs'): 0.5,\n",
       " ('customs', 'union'): 0.5,\n",
       " ('union', ','): 0.5,\n",
       " (',', 'GCC'): 0.5,\n",
       " ('GCC', 'Common'): 0.5,\n",
       " ('Common', 'Market'): 0.5,\n",
       " ...}"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_prob(freq_bi, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppl_interpolation(sentences, lambda_set):\n",
    "    '''Returns the interpolated perplexity of the given list of sentences. Here smoothed values are used.'''\n",
    "    sentences = unigram\n",
    "    counter = 0\n",
    "    tmp = 0\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        for j in range(len(sentence) - 2):\n",
    "            trigram_unit = (sentence[j], sentence[j + 1], sentence[j + 2])\n",
    "            if trigram_unit in trigram_prob:\n",
    "                tri_gram_prob = trigram_prob[trigram_unit]\n",
    "            else:\n",
    "                tri_gram_prob = 0\n",
    "\n",
    "            bigram_unit = (sentence[j], sentence[j + 1])\n",
    "            if bigram_unit in bigram_prob:\n",
    "                bigram_prob = bigram_prob[bigram_unit]\n",
    "            else:\n",
    "                bigram_prob = 0\n",
    "\n",
    "#             unigram_prob = one_gram_add_one_prob.get(sentence[j], 0)\n",
    "\n",
    "            prob = lambda_set[0] * unigram_prob + lambda_set[1] * bigram_prob + lambda_set[2] * tri_gram_prob\n",
    "            if prob == 0:\n",
    "                tmp += 0\n",
    "            else:\n",
    "                tmp += math.log(prob, 2)\n",
    "            counter += 1\n",
    "            \n",
    "            \n",
    "    # entropy = prob of each token / number of tokens\n",
    "    entropy = -1 / counter * tmp\n",
    "    perplexity = math.pow(2, entropy)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5, 0.3, 0.2]"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------INTERPOLATION--------\n",
      "Lambda_Set:  [[0.5, 0.3, 0.2], [0.8, 0.1, 0.1], [0.1, 0.8, 0.1], [0.1, 0.1, 0.8], [0.6, 0.2, 0.2], [0.2, 0.6, 0.2], [0.2, 0.2, 0.6], [0.4, 0.3, 0.3], [0.3, 0.4, 0.3], [0.3, 0.3, 0.4], [0.2, 0.4, 0.4], [0.4, 0.2, 0.4], [0.4, 0.4, 0.2], [0.1, 0.4, 0.5], [0.1, 0.3, 0.6], [0.1, 0.2, 0.7], [0.05, 0.15, 0.8], [0.05, 0.05, 0.9]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'float' and 'FreqDist'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-413-63efe8819287>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlambda_set\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Lambda_Set: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mppl_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mppl_interpolation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"perplexity score of test data:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mppl_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-411-95f623a57b4d>\u001b[0m in \u001b[0;36mppl_interpolation\u001b[0;34m(sentences, lambda_set)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m#             unigram_prob = one_gram_add_one_prob.get(sentence[j], 0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mfreq_uni\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.3\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mfreq_bi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mfreq_tri\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mtmp\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'float' and 'FreqDist'"
     ]
    }
   ],
   "source": [
    "print(\"\\n------INTERPOLATION--------\")\n",
    "lambda_set = []\n",
    "lambda_set.append([0.5, 0.3, 0.2])\n",
    "lambda_set.append([0.8, 0.1, 0.1])\n",
    "lambda_set.append([0.1, 0.8, 0.1])\n",
    "lambda_set.append([0.1, 0.1, 0.8])\n",
    "lambda_set.append([0.6, 0.2, 0.2])\n",
    "lambda_set.append([0.2, 0.6, 0.2])\n",
    "lambda_set.append([0.2, 0.2, 0.6])\n",
    "lambda_set.append([0.4, 0.3, 0.3])\n",
    "lambda_set.append([0.3, 0.4, 0.3])\n",
    "lambda_set.append([0.3, 0.3, 0.4])\n",
    "lambda_set.append([0.2, 0.4, 0.4])\n",
    "lambda_set.append([0.4, 0.2, 0.4])\n",
    "lambda_set.append([0.4, 0.4, 0.2])\n",
    "lambda_set.append([0.1, 0.4, 0.5])\n",
    "lambda_set.append([0.1, 0.3, 0.6])\n",
    "lambda_set.append([0.1, 0.2, 0.7])\n",
    "lambda_set.append([0.05, 0.15, 0.8])\n",
    "lambda_set.append([0.05, 0.05, 0.9])\n",
    "for s in lambda_set:\n",
    "    print(\"Lambda_Set: \", lambda_set)\n",
    "    ppl_score = ppl_interpolation(train, s)\n",
    "    print(\"perplexity score of test data:\", ppl_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (<ipython-input-392-a6d4c3f1a007>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-392-a6d4c3f1a007>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    return { n_gram: Laplace_smoothing(n_gram, count) for n_gram, count in n_vocab.items() }\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    " entropy = -1 / counter * tmp\n",
    "        # print(\"entropy\", entropy)\n",
    "        perplexity = math.pow(2, entropy)\n",
    "        return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-8-7f828843f24a>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-7f828843f24a>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    print(\"RNN_LanguageModel (drop): Dropout(p=0.5)(encoder): embedding(33278, 650) (RNN): lstm(650, 650, num_layers=2, dropout=0.5)\u001b[0m\n\u001b[0m                                                                                                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "def rnnmodel_evaluation(datapath):\n",
    "    model.eval()\n",
    "    loss = 0.\n",
    "    num_of_tokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(10)\n",
    "    with torch.no_grad():\n",
    "        for k in range(0, datapath.size(0) - 1, BPTT):\n",
    "            data, result = get_batch(datapath, k)\n",
    "            output, hidden = model(data, hidden)\n",
    "            output = output.view(-1, num_of_tokens)\n",
    "            total_loss += len(data) * CRITERION(output, result).item()\n",
    "            hidden = repackage_hidden(hidden)\n",
    "    return total_loss / len(datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
