{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import math\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import operator\n",
    "from collections import defaultdict as dd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-101-32cd5041a44b>:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  masked_txt = pd.read_csv (r'/Users/nehakardam/Documents/UWclasses /CSE NLP/A6/data_A6/15pctmasked.txt',sep='delimiter', names = [\"Sentences\"])\n",
      "<ipython-input-101-32cd5041a44b>:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  lm_txt = pd.read_csv (r'/Users/nehakardam/Documents/UWclasses /CSE NLP/A6/data_A6/lm.txt', sep='delimiter', names = [\"Words\", \"Prob\"])\n"
     ]
    }
   ],
   "source": [
    "masked_txt = pd.read_csv (r'/Users/nehakardam/Documents/UWclasses /CSE NLP/A6/data_A6/15pctmasked.txt',sep='delimiter', names = [\"Sentences\"])\n",
    "masked_txt.to_csv (r'/Users/nehakardam/Documents/UWclasses /CSE NLP/A6/data_A6/15pctmasked.csv')\n",
    "lm_txt = pd.read_csv (r'/Users/nehakardam/Documents/UWclasses /CSE NLP/A6/data_A6/lm.txt', sep='delimiter', names = [\"Words\", \"Prob\"])\n",
    "lm_txt.to_csv (r'/Users/nehakardam/Documents/UWclasses /CSE NLP/A6/data_A6/lm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Viterbi Algorithm: 100%|██████████| 5785/5785 [05:07<00:00, 18.81it/s]\n"
     ]
    }
   ],
   "source": [
    "blm = BigramModel('/Users/nehakardam/Documents/UWclasses /CSE NLP/A6/data_A6/lm.txt')\n",
    "v = Viterbi('/Users/nehakardam/Documents/UWclasses /CSE NLP/A6/data_A6/15pctmasked.txt', \n",
    "            '/Users/nehakardam/Documents/UWclasses /CSE NLP/A6/data_A6/unmasked.txt')\n",
    "complete_sentences = v.compute_missing_characters(blm)\n",
    "v.write_sentences_to_file(complete_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "START = '<start>'\n",
    "MASK = '<mask>'\n",
    "SPACE = '<s>'\n",
    "EOS = '<eos>'\n",
    "\n",
    "class BigramModel:\n",
    "    def __init__(self, file_path):\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Path: {file_path} does not exist\")\n",
    "            return None\n",
    "        \n",
    "        with open(file_path, \"r\") as f:\n",
    "            bigrams = f.readlines()\n",
    "        \n",
    "        self.blm = dd(dict)\n",
    "\n",
    "        for bigram in bigrams:\n",
    "            bigram, prob = bigram.split('\\t')[0], bigram.split('\\t')[1]\n",
    "            \n",
    "            w1, w2 = bigram.split(' ')[0], bigram.split(' ')[1]\n",
    "            prob = float(prob.strip())\n",
    "\n",
    "            self.blm[w1][w2] = prob\n",
    "    \n",
    "        self._create_labels_dict()\n",
    "\n",
    "    def _create_labels_dict(self):\n",
    "        labels = list(self.blm.keys())\n",
    "        labels.sort()\n",
    "        labels.append(labels.pop(labels.index(EOS))) # add EOS to end\n",
    "        labels = [labels.pop(labels.index(START))] + labels # add START to beginning\n",
    "\n",
    "        self.labels = labels\n",
    "        self.label_to_idx = {k: v for v, k in enumerate(labels)}\n",
    "        self.idx_to_label = {v: k for v, k in enumerate(labels)}\n",
    "\n",
    "    def get_labels(self):\n",
    "        return self.labels\n",
    "\n",
    "    def get_labels_to_index(self):\n",
    "        return self.label_to_idx\n",
    "    \n",
    "    def get_index_to_labels(self):\n",
    "        return self.idx_to_label\n",
    "            \n",
    "    def get_bigram_prob(self, w1, w2) -> float:\n",
    "        if w1 in self.blm.keys() and w2 in self.blm[w1].keys():\n",
    "            return self.blm[w1][w2] # p(w2 | w1) = prob\n",
    "        return 0\n",
    "    \n",
    "    def get_w2_given_w1(self, w1, w2, is_log_prob=True):\n",
    "        if w1 in self.blm.keys() and w2 in self.blm[w1].keys():\n",
    "            if is_log_prob: return np.log(self.blm[w1][w2]) # p(w2 | w1) = ln(prob)\n",
    "            else: return self.blm[w1][w2] # p(w2 | w1) = ln(prob)\n",
    "\n",
    "        if is_log_prob: return float('-inf')\n",
    "        return 0\n",
    "    \n",
    "    def get_max_from_key(self, w1) -> str:\n",
    "        return max(self.blm[w1].items(), key=operator.itemgetter(1))\n",
    "\n",
    "class Viterbi:\n",
    "    def __init__(self, input_file_path, output_file_path):\n",
    "        if not os.path.exists(input_file_path):\n",
    "            print(f\"Path: {input_file_path} does not exist\")\n",
    "            return None\n",
    "        \n",
    "        # Read and parse the input file\n",
    "        with open(input_file_path, \"r\") as f:\n",
    "            l = f.readlines()\n",
    "        \n",
    "        self.masked_sentences = []\n",
    "        for sentence in l:\n",
    "            sentence = sentence.split()\n",
    "            self.masked_sentences.append(sentence)\n",
    "\n",
    "        self.output_file_path = output_file_path\n",
    "\n",
    "    def compute_missing_characters(self, blm: BigramModel):\n",
    "        states = blm.get_labels()\n",
    "        complete_sentences = []\n",
    "\n",
    "        for sentence in tqdm(self.masked_sentences, desc=\"Running Viterbi Algorithm\"):\n",
    "            best_path = self.viterbi_algorithm(sentence, states, blm)\n",
    "            complete_sentences.append(best_path)\n",
    "        \n",
    "        return complete_sentences\n",
    "\n",
    "    def viterbi_algorithm(self, observation, states, blm: BigramModel):\n",
    "\n",
    "        sentence = observation[:]\n",
    "\n",
    "        idx_to_state = blm.get_index_to_labels()\n",
    "        state_to_idx = blm.get_labels_to_index()\n",
    "        R, C = len(states), len(sentence)\n",
    "\n",
    "        # To hold p. of each state given each sentence.\n",
    "        trellis = np.full((R, C), -np.inf)\n",
    "        \n",
    "        # to hold the back pointers for cell\n",
    "        back_pointer = np.zeros((R, C), dtype='int32')\n",
    "\n",
    "        # Determine each hidden state's p. at time 0\n",
    "        for i in range(R-1):\n",
    "            if states[i] == START:\n",
    "                trellis[i][0] = blm.get_w2_given_w1(START, START) # initial probability of START symbol\n",
    "        \n",
    "        # and now, assuming each state's most likely prior state, k\n",
    "        for j in range(1, C-1):\n",
    "            w1, w2 = sentence[j-1], sentence[j]\n",
    "\n",
    "            for i in range(R-1):\n",
    "                label = states[i]\n",
    "                \n",
    "                # Case 1: w1 and w2 are both known characters\n",
    "                if w1 != MASK and w2 != MASK:\n",
    "                    label_idx = state_to_idx[w2]\n",
    "                    max_prev_trellis_value = max(trellis[:, j-1])\n",
    "                    max_prev_trellis_label_idx = np.argmax(trellis[:, j-1])\n",
    "\n",
    "                    trellis[label_idx][j] = max_prev_trellis_value + blm.get_w2_given_w1(w1=idx_to_state[max_prev_trellis_label_idx], w2=w2)\n",
    "                    back_pointer[label_idx][j] = max_prev_trellis_label_idx\n",
    "                    break\n",
    "                \n",
    "                # Case 2: curr is MASK and prev column is known\n",
    "                elif w1 != MASK and w2 == MASK:\n",
    "                    \n",
    "                    max_prev_trellis_value = max(trellis[:, j-1])\n",
    "                    max_prev_trellis_label_idx = np.argmax(trellis[:, j-1])\n",
    "\n",
    "                    trellis[i][j] = max_prev_trellis_value + blm.get_w2_given_w1(w1=idx_to_state[max_prev_trellis_label_idx], w2=label)\n",
    "                    back_pointer[i][j] = max_prev_trellis_label_idx\n",
    "\n",
    "                # Case 3: curr is known and prev column is MASK\n",
    "                elif w2 != MASK and w1 == MASK:\n",
    "                    label_idx = state_to_idx[w2]\n",
    "\n",
    "                    t1 = np.full((R-1, ), -np.inf)\n",
    "                    for k in range(R-1):\n",
    "                        prev_label = states[k]\n",
    "                        t1[k] = blm.get_w2_given_w1(w1=prev_label, w2=w2) + trellis[k][j-1]\n",
    "                    \n",
    "                    trellis[label_idx][j] = max(t1)\n",
    "                    back_pointer[label_idx][j] = np.argmax(t1)\n",
    "                    break\n",
    "                \n",
    "                # Case 4: w1 and w2 are both MASK characters\n",
    "                else:\n",
    "                    t1 = np.full((R-1, ), -np.inf)\n",
    "\n",
    "                    '''\n",
    "                    Since the prev column is also a mask, we pick the current label and iterate through\n",
    "                    all the other lables as well. Then find the max\n",
    "                    '''\n",
    "                    for k in range(R-1):\n",
    "                        prev_label = states[k]\n",
    "                        t1[k] = blm.get_w2_given_w1(w1=prev_label, w2=label) + trellis[k][j-1]\n",
    "                    trellis[i][j] = max(t1)\n",
    "                    back_pointer[i][j] = np.argmax(t1)\n",
    "            \n",
    "        # Fill in the prob for <eos>\n",
    "        trellis[R-1][C-1] = blm.get_w2_given_w1(w1=EOS, w2=EOS)\n",
    "        back_pointer[R-1][C-1] = np.argmax(trellis[:, C-2])\n",
    "\n",
    "        # np.savetxt('back_pointer.out', np.vstack((['header'] + sentence, np.column_stack((states, back_pointer.round(decimals=0))))), fmt=\"%-12s\")\n",
    "        # np.savetxt('trellis.out', np.vstack((['header'] + sentence, np.column_stack((states, trellis.round(decimals=4))))), fmt=\"%-12s\")\n",
    "\n",
    "        # get the back pointers \n",
    "        guessed_sentence = [EOS]\n",
    "        label_idx = back_pointer[R-1][C-1]\n",
    "        for j in reversed(range(C-1)):\n",
    "            guessed_sentence.append(states[label_idx])\n",
    "            label_idx = back_pointer[label_idx][j]\n",
    "        \n",
    "        return list(reversed(guessed_sentence))\n",
    "\n",
    "    def write_sentences_to_file(self, sentence_list):\n",
    "        sentence_list_strings = []\n",
    "        \n",
    "        for sentence in sentence_list:\n",
    "            s = ' '.join(sentence)\n",
    "            sentence_list_strings.append(s)\n",
    "        \n",
    "        s = '\\n'.join(sentence_list_strings)\n",
    "\n",
    "        with open(self.output_file_path, \"w\") as f:\n",
    "            f.write(s)\n",
    "\n",
    "def sanity_check_output(masked_sentences, un_masked_sentences):\n",
    "    print(\"Performing sanity check on output\")\n",
    "\n",
    "    # Ensure the first sentence matches the correct output:\n",
    "    correct_out = ['<start>', 'I', '<s>', 'p', 'e', '<s>', 'm', 'a', 'n', 't', 'a', 't', 'i', 'o', 'n', '<s>', 'o', 'f', '<s>', 'G', 'e', 'o', 'r', 'g', 'i', 'a', \"'\", \"'\", '<s>', 'a', 'u', 'r', 'o', 'm', 'o', 'b', 'i', 'l', 'e', '<s>', 't', 'i', 't', 'l', 'e', '<s>', 'l', 'a', 'w', '<s>', 'w', 'a', 's', '<s>', 'a', 'l', '<s>', ',', '<s>', 'h', 'e', 'c', 'o', 'm', 'm', 'e', 'n', 'd', 'e', 'd', '<s>', 'b', 'e', '<s>', 't', 'h', 'e', '<s>', 'o', 'u', 't', 'g', 'o', 'i', 'n', 'g', '<s>', 'j', 'u', 'r', 'y', '<s>', '.', '<eos>']\n",
    "    for correct_char, unmasked_char in zip(correct_out, un_masked_sentences[0]):\n",
    "        if correct_char != unmasked_char:\n",
    "            print(f'Error! Viterbi Algorithm is incorrect for sentence 1 Correct: {correct_char} : Unmasked {unmasked_char}')\n",
    "            return\n",
    "\n",
    "    for masked_sentence, unmasked_sentence in zip(masked_sentences, un_masked_sentences):\n",
    "\n",
    "        # Ensure the length of 2 sentence is the same (same number of chars)\n",
    "        lm, lum = len(masked_sentence), len(unmasked_sentence)\n",
    "        if lm != lum:\n",
    "            print(f\"Error! The length of the sentences do not match\")\n",
    "            print(f\"Masked Sentence: {masked_sentence}\")\n",
    "            print(f\"Unmasked Sentence: {unmasked_sentence}\")\n",
    "        \n",
    "        # Ensure we only changed the <mask> characters\n",
    "        for i in range(lm):\n",
    "            c_m, c_um = masked_sentence[i], unmasked_sentence[i]\n",
    "\n",
    "            if c_m != MASK and (c_m != c_um):\n",
    "                print(f\"Error! Changed a known character\")\n",
    "                print(f\"Changed {c_m} -> {c_um} at index: {i}\")\n",
    "            \n",
    "            elif c_m == MASK and c_um == START:\n",
    "                print(f\"Changed a masked char to <start> token!\")\n",
    "                print(masked_sentence)\n",
    "                print(unmasked_sentence)\n",
    "                print(\"\")\n",
    "            \n",
    "def parse_output_file(output_file_path):\n",
    "    un_masked_sentences = []\n",
    "\n",
    "    # Read in output file if it exists\n",
    "    if os.path.exists(output_file_path):\n",
    "        with open(output_file_path, \"r\") as f:\n",
    "            l = f.readlines()\n",
    "        for sentence in l:\n",
    "            sentence = sentence.split()\n",
    "            un_masked_sentences.append(sentence)\n",
    "    \n",
    "    return un_masked_sentences\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     print(\"NLP - A4\")\n",
    "\n",
    "#     parser = argparse.ArgumentParser(description='Viterbi Algorithm')\n",
    "#     parser.add_argument(\"-lm\", \"--lang-model\", dest=\"lang_model_path\", type=str, default=\"./lm.txt\", required=False, help='This is the path to the language model file')\n",
    "#     parser.add_argument(\"-ip\", \"--input-file\", dest=\"input_file_path\", type=str, default=\"./15pctmasked.txt\", required=False, help='This is the path to the file that contains the masked sentences')\n",
    "#     parser.add_argument(\"-op\", \"--output-file\", dest=\"output_file_path\", type=str, default=\"./unmasked.txt\", required=False, help='This is the path to file that will be output')\n",
    "#     parser.add_argument(\"-t\", \"--sanity-check\", dest=\"perform_sanity_check\", action='store_true', help='Flag to indicate whether to perform sanity checking our not')\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "#     blm = BigramModel(args.lang_model_path)\n",
    "#     v = Viterbi(input_file_path=args.input_file_path, output_file_path=args.output_file_path)\n",
    "\n",
    "#     complete_sentences = v.compute_missing_characters(blm)\n",
    "#     v.write_sentences_to_file(complete_sentences)\n",
    "\n",
    "#     if args.perform_sanity_check:\n",
    "#         sanity_check_output(v.masked_sentences, parse_output_file(args.output_file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
